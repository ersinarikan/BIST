# Filtreye TakÄ±lan Durumlarda Model EÄŸitimi - Parametreler

## ğŸ¯ Soru: Filtreye TakÄ±lan Durumlarda Model Hangi Parametrelerle EÄŸitilir?

## 1. HPO SÃ¼recinde (optuna_hpo_with_feature_flags.py)

### Senaryo: Bir sembol iÃ§in tÃ¼m split'ler filtreye takÄ±lÄ±rsa

**Kod AkÄ±ÅŸÄ±:**
1. Her split iÃ§in model eÄŸitilir (satÄ±r 630):
   ```python
   result = ml.train_enhanced_models(sym, train_df)
   ```

2. Model eÄŸitimi **trial'Ä±n Ã¶nerdiÄŸi parametrelerle** yapÄ±lÄ±r:
   - Trial parametreleri (satÄ±r 302-600): `trial.suggest_*`
   - Feature flags (satÄ±r 303-316): `trial.suggest_categorical('enable_*', ...)`
   - Model choice (satÄ±r 318-340): `trial.suggest_categorical('model_choice', ...)`
   - Hyperparameters (satÄ±r 350-600): `trial.suggest_*` (xgb_*, lgb_*, cat_*)

3. EÄŸer bir sembol iÃ§in tÃ¼m split'ler filtreye takÄ±lÄ±rsa:
   - O sembol score'a dahil edilmez (satÄ±r 884-885)
   - **Ama model yine de eÄŸitilir** (her split iÃ§in)
   - **Parametreler**: Trial'Ä±n Ã¶nerdiÄŸi parametreler (trial.suggest_*)

**SonuÃ§**: HPO sÄ±rasÄ±nda filtreye takÄ±lan semboller iÃ§in model **trial parametreleriyle** eÄŸitilir, ama score'a dahil edilmez.

### Best Params SeÃ§imi (satÄ±r 1135-1250)

**Kod:**
```python
best_params = study.best_params  # TÃ¼m sembollerin ortalamasÄ± Ã¼zerinden seÃ§ilir
best_trial = study.best_trial
```

**MantÄ±k:**
- Best params **tÃ¼m sembollerin ortalamasÄ±** Ã¼zerinden seÃ§ilir
- EÄŸer bir sembol iÃ§in tÃ¼m split'ler filtreye takÄ±lÄ±rsa:
  - O sembol score'a dahil edilmez
  - Ama diÄŸer semboller varsa â†’ OnlarÄ±n ortalamasÄ± alÄ±nÄ±r
  - Best params **diÄŸer semboller iÃ§in** optimal olur

**âš ï¸  Sorun**: EÄŸer bir sembol iÃ§in tÃ¼m split'ler filtreye takÄ±lÄ±rsa, o sembol iÃ§in best params **optimal olmayabilir** (Ã§Ã¼nkÃ¼ o sembol score'a dahil edilmemiÅŸ).

## 2. Training SÃ¼recinde (continuous_hpo_training_pipeline.py)

### Senaryo: TÃ¼m split'ler filtreye takÄ±lÄ±rsa

**Kod AkÄ±ÅŸÄ±:**
1. Best params JSON dosyasÄ±ndan okunur (satÄ±r 3651):
   ```python
   best_params_with_trial = hpo_result['best_params'].copy()
   ```

2. Best params environment variable'larÄ±na set edilir (satÄ±r 3015):
   ```python
   set_hpo_params_as_env(best_params, horizon)
   ```

3. Model eÄŸitimi (satÄ±r 3213):
   ```python
   result = ml.train_enhanced_models(symbol, df)
   ```

4. EÄŸer tÃ¼m split'ler filtreye takÄ±lÄ±rsa:
   - Model yine de eÄŸitilir âœ…
   - **Parametreler**: Best params (HPO'dan gelen)
   - DirHit None olur (hesaplanamaz)

**SonuÃ§**: Training sÄ±rasÄ±nda filtreye takÄ±lan semboller iÃ§in model **best params ile** eÄŸitilir, ama DirHit hesaplanamaz.

## ğŸ” DetaylÄ± Parametre Setleme

### set_hpo_params_as_env (train_completed_hpo_with_best_params.py, satÄ±r 80-210)

**YapÄ±lan Ä°ÅŸlemler:**
1. **Feature Flags** (satÄ±r 85-100):
   ```python
   os.environ['ENABLE_EXTERNAL_FEATURES'] = str(params.get('enable_external_features', True))
   os.environ['ENABLE_FINGPT_FEATURES'] = str(params.get('enable_fingpt_features', True))
   # ... diÄŸer feature flags
   ```

2. **Model Choice** (satÄ±r 102-110):
   ```python
   model_choice = params.get('model_choice', 'all')
   os.environ['ENABLE_XGBOOST'] = '1' if model_choice in ('xgb', 'all') else '0'
   os.environ['ENABLE_LIGHTGBM'] = '1' if model_choice in ('lgbm', 'all') else '0'
   os.environ['ENABLE_CATBOOST'] = '1' if model_choice in ('cat', 'all') else '0'
   ```

3. **Hyperparameters** (satÄ±r 112-200):
   ```python
   # XGBoost params
   for key, value in params.items():
       if key.startswith('xgb_'):
           os.environ[f'OPTUNA_XGB_{key[4:].upper()}'] = str(value)
   # LightGBM params
   for key, value in params.items():
       if key.startswith('lgb_'):
           os.environ[f'OPTUNA_LGB_{key[4:].upper()}'] = str(value)
   # CatBoost params
   for key, value in params.items():
       if key.startswith('cat_'):
           os.environ[f'OPTUNA_CAT_{key[4:].upper()}'] = str(value)
   ```

4. **Feature Parameters** (satÄ±r 3026-3043):
   ```python
   # Smart-ensemble params
   if 'smart_consensus_weight' in fp:
       os.environ['ML_SMART_CONSENSUS_WEIGHT'] = str(fp['smart_consensus_weight'])
   # ... diÄŸer feature params
   ```

## âš ï¸  Kritik Sorun

### Senaryo: Bir sembol iÃ§in tÃ¼m split'ler filtreye takÄ±lÄ±rsa

**HPO'da:**
- Model **trial parametreleriyle** eÄŸitilir
- O sembol score'a dahil edilmez
- Best params **diÄŸer semboller iÃ§in** optimal olur

**Training'de:**
- Model **best params ile** eÄŸitilir (diÄŸer semboller iÃ§in optimal)
- **Ama bu sembol iÃ§in optimal olmayabilir!** âš ï¸

**SonuÃ§**: EÄŸer bir sembol iÃ§in tÃ¼m split'ler filtreye takÄ±lÄ±rsa, o sembol iÃ§in best params **optimal olmayabilir** (Ã§Ã¼nkÃ¼ o sembol HPO score hesaplamasÄ±na dahil edilmemiÅŸ).

## âœ… Ã‡Ã¶zÃ¼m Ã–nerileri

1. **Filtreyi gevÅŸetmek**: 10/5.0 â†’ 5/3.0 veya 0/0.0
2. **Sembol-spesifik filtre**: BazÄ± semboller iÃ§in farklÄ± filtre deÄŸerleri
3. **Best params seÃ§imini deÄŸiÅŸtirmek**: Sadece geÃ§erli semboller iÃ§in best params seÃ§mek
4. **UyarÄ± mekanizmasÄ±**: Filtreye takÄ±lan semboller iÃ§in uyarÄ± vermek

## ğŸ“Š Ã–zet Tablo

| Durum | HPO'da Parametreler | Training'de Parametreler |
|-------|---------------------|--------------------------|
| TÃ¼m split'ler geÃ§er | Trial params â†’ Best params | Best params âœ… |
| BazÄ± split'ler geÃ§er | Trial params â†’ Best params | Best params âœ… |
| HiÃ§bir split geÃ§emez | Trial params (score'a dahil deÄŸil) | Best params (optimal olmayabilir) âš ï¸ |

