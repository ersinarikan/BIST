"""
BIST Automated Data Pipeline Scheduler
GÃ¼nlÃ¼k otomatik veri toplama, model eÄŸitimi ve sistem izleme
"""

import schedule
import time
import logging
import threading
from datetime import datetime, timedelta
import os
import json
import pandas as pd
# Local imports
try:
    from data_collector import get_data_collector
    from ml_prediction_system import get_ml_prediction_system
    from simple_enhanced_ml import get_simple_enhanced_ml
    from enhanced_ml_system import get_enhanced_ml_system
    from alert_system import get_alert_system
    SYSTEMS_AVAILABLE = True
except ImportError as e:
    SYSTEMS_AVAILABLE = False
    print(f"âš ï¸ System import error: {e}")

# Email imports (optional, don't break SYSTEMS_AVAILABLE)
try:
    import smtplib
    from email.mime.text import MIMEText
    from email.mime.multipart import MIMEMultipart
    EMAIL_AVAILABLE = True
except ImportError as e:
    EMAIL_AVAILABLE = False
    print(f"âš ï¸ Email system not available: {e}")

logger = logging.getLogger(__name__)

class AutomatedDataPipeline:
    """Otomatik veri pipeline'Ä± ve sistem yÃ¶netimi"""
    
    def __init__(self):
        self.is_running = False
        self.last_run_stats = {}
        self.scheduler_thread = None
        self.performance_threshold = 0.7  # %70 altÄ±ndaki modeller yeniden eÄŸitilir
        # Watchdog: thread Ã¶lÃ¼rse yeniden baÅŸlatma iÃ§in throttle timestamp
        self.last_watchdog_restart_ts = 0.0
        # Idle watchdog: en son etkinlik zaman damgasÄ± ve kullanÄ±cÄ± kaynaklÄ± durdurma bayraÄŸÄ±
        self.last_activity_ts = 0.0
        self.user_stopped = False
        self._idle_watchdog_thread = None
        
        # Email settings (opsiyonel)
        self.email_enabled = False
        self.email_settings = {
            'smtp_server': 'smtp.gmail.com',
            'smtp_port': 587,
            'username': '',
            'password': '',
            'to_emails': []
        }
        
        logger.info("ğŸ¤– Automated Data Pipeline baÅŸlatÄ±ldÄ±")

    def _get_stock_dataframe(self, symbol: str):
        """PostgreSQL'den bir hissenin OHLCV verisini DataFrame olarak getir (index=date)."""
        try:
            from app import app as flask_app
            from models import Stock, StockPrice
            with flask_app.app_context():
                stock = Stock.query.filter_by(symbol=symbol).first()
                if not stock:
                    return None
                prices = StockPrice.query.filter_by(stock_id=stock.id)\
                    .order_by(StockPrice.date.asc()).all()
                if not prices:
                    return None
                rows = []
                for p in prices:
                    rows.append({
                        'date': p.date,
                        'open': float(p.open_price),
                        'high': float(p.high_price),
                        'low': float(p.low_price),
                        'close': float(p.close_price),
                        'volume': int(p.volume)
                    })
                df = pd.DataFrame(rows)
                if df.empty:
                    return None
                df['date'] = pd.to_datetime(df['date'])
                df.set_index('date', inplace=True)
                return df
        except Exception as e:
            logger.error(f"DF fetch error {symbol}: {e}")
            return None

    def run_bulk_predictions_all(self) -> dict | bool:
        """TÃ¼m aktif hisseler iÃ§in 1/3/7/14/30 gÃ¼nlÃ¼k tahminleri Ã¼ret ve kaydet.

        - Temel ML her zaman Ã§alÄ±ÅŸÄ±r (hÄ±zlÄ±)
        - ENV: ENABLE_ENHANCED_ML=True ise Enhanced ML de eÄŸitim+tahmin yapar
        - SonuÃ§lar: /opt/bist-pattern/logs/ml_bulk_predictions.json
        """
        try:
            # UI'ya bilgi amaÃ§lÄ± yayÄ±n
            try:
                from app import app as flask_app
                if hasattr(flask_app, 'broadcast_log'):
                    flask_app.broadcast_log('INFO', 'ğŸ¤– ML bulk predictions starting...', 'ml')
            except Exception:
                pass

            # Semboller
            from app import app as flask_app
            with flask_app.app_context():
                from models import Stock
                symbols = [s.symbol for s in Stock.query.filter_by(is_active=True).all()]

            # ML sistemleri
            basic = None
            try:
                from ml_prediction_system import get_ml_prediction_system
                basic = get_ml_prediction_system()
            except Exception:
                basic = None
            use_enhanced = str(os.getenv('ENABLE_ENHANCED_ML', 'false')).lower() in ('1', 'true', 'yes')
            enhanced = None
            if use_enhanced:
                try:
                    from enhanced_ml_system import get_enhanced_ml_system
                    enhanced = get_enhanced_ml_system()
                except Exception:
                    enhanced = None

            results: dict = {'timestamp': datetime.now().isoformat(), 'predictions': {}}
            processed = 0
            for sym in symbols:
                try:
                    df = self._get_stock_dataframe(sym)
                    if df is None or len(df) < 50:
                        continue
                    out_sym: dict = {}
                    if basic is not None:
                        try:
                            preds = basic.predict_prices(sym, df, None) or {}
                            out_sym['basic'] = preds
                        except Exception:
                            pass
                    if enhanced is not None and len(df) >= 200:
                        try:
                            enhanced.train_enhanced_models(sym, df)
                            ep = enhanced.predict_enhanced(sym, df) or {}
                            out_sym['enhanced'] = ep
                        except Exception:
                            pass
                    if out_sym:
                        results['predictions'][sym] = out_sym
                        processed += 1
                except Exception:
                    continue

            # Kaydet
            try:
                log_dir = '/opt/bist-pattern/logs'
                os.makedirs(log_dir, exist_ok=True)
                fp = os.path.join(log_dir, 'ml_bulk_predictions.json')
                with open(fp, 'w') as wf:
                    json.dump(results, wf)
            except Exception:
                pass

            # UI yayÄ±n
            try:
                from app import app as flask_app
                if hasattr(flask_app, 'broadcast_log'):
                    flask_app.broadcast_log('SUCCESS', f'âœ… ML bulk predictions completed: {processed} symbols', 'ml')
            except Exception:
                pass
            return results
        except Exception as e:
            logger.error(f"ML bulk predictions error: {e}")
            return False
    
    def daily_data_collection(self):
        """GÃ¼nlÃ¼k veri toplama gÃ¶revi"""
        try:
            logger.info("ğŸ“… GÃ¼nlÃ¼k veri toplama baÅŸlatÄ±lÄ±yor...")
            
            if not SYSTEMS_AVAILABLE:
                logger.error("âŒ Sistem modÃ¼lleri mevcut deÄŸil")
                return False
            
            # Import app locally to avoid circular imports
            from app import app
            with app.app_context():
                collector = get_data_collector()
                
                # GÃ¼ncel veri toplama (son 7 gÃ¼n)
                symbols = collector.get_bist_symbols()
                updated_count = 0
                failed_count = 0
                
                for symbol in symbols[:20]:  # Ä°lk 20 hisse ile baÅŸla
                    try:
                        success = collector.update_single_stock(symbol, days=7)
                        if success:
                            updated_count += 1
                        else:
                            failed_count += 1
                        
                        # Rate limiting
                        time.sleep(0.5)
                        
                    except Exception as e:
                        logger.error(f"âŒ {symbol} gÃ¼nlÃ¼k gÃ¼ncelleme hatasÄ±: {e}")
                        failed_count += 1
                
                # Ä°statistikleri kaydet
                stats = {
                    'date': datetime.now().isoformat(),
                    'updated_stocks': updated_count,
                    'failed_stocks': failed_count,
                    'total_processed': updated_count + failed_count
                }
                
                self.last_run_stats['data_collection'] = stats
                logger.info(f"âœ… GÃ¼nlÃ¼k veri toplama tamamlandÄ±: {updated_count} baÅŸarÄ±lÄ±, {failed_count} hata")
                
                return True
                
        except Exception as e:
            logger.error(f"âŒ GÃ¼nlÃ¼k veri toplama hatasÄ±: {e}")
            return False
    
    def weekly_full_collection(self):
        """HaftalÄ±k tam veri toplama"""
        try:
            logger.info("ğŸ“… HaftalÄ±k tam veri toplama baÅŸlatÄ±lÄ±yor...")
            
            if not SYSTEMS_AVAILABLE:
                logger.error("âŒ Sistem modÃ¼lleri mevcut deÄŸil")
                return False
            
            # Import app locally to avoid circular imports
            from app import app
            with app.app_context():
                collector = get_data_collector()
                
                # Tam veri toplama (son 1 ay)
                result = collector.collect_all_data(max_workers=3, period="1mo")
                
                if result:
                    self.last_run_stats['weekly_collection'] = result
                    logger.info(f"âœ… HaftalÄ±k tam veri toplama tamamlandÄ±: {result}")
                    return True
                else:
                    logger.error("âŒ HaftalÄ±k veri toplama baÅŸarÄ±sÄ±z")
                    return False
                    
        except Exception as e:
            logger.error(f"âŒ HaftalÄ±k veri toplama hatasÄ±: {e}")
            return False
    
    # MIGRATED TO DAEMON: auto_model_retraining, check_model_performance
    # These functions are now handled by scheduler_daemon.py
    
    # MIGRATED TO DAEMON: system_health_check  
    # This function is now handled by scheduler_daemon.py
    
    def send_status_email(self, subject, content):
        """Durum raporu email gÃ¶nder (opsiyonel)"""
        try:
            if not EMAIL_AVAILABLE:
                logger.warning("ğŸ“§ Email system not available, skipping email")
                return True
                
            if not self.email_enabled or not self.email_settings['to_emails']:
                return True  # Email devre dÄ±ÅŸÄ±
            
            msg = MIMEMultipart()
            msg['From'] = self.email_settings['username']
            msg['To'] = ', '.join(self.email_settings['to_emails'])
            msg['Subject'] = f"BIST AI System - {subject}"
            
            msg.attach(MIMEText(content, 'plain', 'utf-8'))
            
            server = smtplib.SMTP(self.email_settings['smtp_server'], self.email_settings['smtp_port'])
            server.starttls()
            server.login(self.email_settings['username'], self.email_settings['password'])
            
            text = msg.as_string()
            server.sendmail(self.email_settings['username'], self.email_settings['to_emails'], text)
            server.quit()
            
            logger.info(f"ğŸ“§ Status email sent: {subject}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Email gÃ¶nderme hatasÄ±: {e}")
            return False
    
    def daily_status_report(self):
        """GÃ¼nlÃ¼k durum raporu"""
        try:
            logger.info("ğŸ“Š GÃ¼nlÃ¼k durum raporu oluÅŸturuluyor...")
            
            # SAFE: Skip health check to avoid Flask context issues
            # health_status = self.system_health_check()  # This kills the thread!
            health_status = {'overall_status': 'unknown', 'systems': {}}
            
            # Rapor oluÅŸtur
            report = f"""
ğŸ¤– BIST AI System Daily Report - {datetime.now().strftime('%Y-%m-%d %H:%M')}

ğŸ“Š System Health: {health_status.get('overall_status', 'unknown')}

ğŸ“ˆ Data Collection Stats:
{json.dumps(self.last_run_stats.get('data_collection', {}), indent=2)}

ğŸ§  Model Training Stats:
{json.dumps(self.last_run_stats.get('model_retraining', {}), indent=2)}

ğŸ” Health Check Results:
{json.dumps(health_status.get('systems', {}), indent=2)}

---
BIST Automated Data Pipeline
            """
            
            logger.info("ğŸ“‹ GÃ¼nlÃ¼k rapor oluÅŸturuldu")
            
            # Email gÃ¶nder (eÄŸer aktifse)
            if health_status.get('overall_status') in ['warning', 'error']:
                self.send_status_email("System Alert", report)
            
            return report
            
        except Exception as e:
            logger.error(f"âŒ GÃ¼nlÃ¼k rapor hatasÄ±: {e}")
            return None
    
    def system_health_check(self):
        """Basit saÄŸlÄ±k kontrolÃ¼: DB istatistikleri ve disk boÅŸ alanÄ±nÄ± yaz."""
        try:
            logger.info("ğŸ” SaÄŸlÄ±k kontrolÃ¼ (internal)")
            health_status = {
                'timestamp': datetime.now().isoformat(),
                'systems': {},
                'overall_status': 'healthy'
            }
            # Data collection stats
            try:
                from app import app as flask_app
                with flask_app.app_context():
                    stats = get_data_collector().get_collection_stats()
                health_status['systems']['data_collection'] = {
                    'status': 'healthy' if (isinstance(stats, dict) and stats.get('total_stocks', 0) > 0) else 'warning',
                    'details': stats
                }
            except Exception as e:
                health_status['systems']['data_collection'] = {'status': 'error', 'details': str(e)}
            # Disk space
            try:
                import shutil
                free_gb = shutil.disk_usage('/').free / (1024**3)
                health_status['systems']['disk_space'] = {
                    'status': 'healthy' if free_gb > 5 else 'warning',
                    'details': f"{free_gb:.1f} GB free"
                }
            except Exception as e:
                health_status['systems']['disk_space'] = {'status': 'error', 'details': str(e)}
            # Overall
            if any(s.get('status') == 'error' for s in health_status['systems'].values()):
                health_status['overall_status'] = 'error'
            elif any(s.get('status') == 'warning' for s in health_status['systems'].values()):
                health_status['overall_status'] = 'warning'
            # Persist JSON (for dashboard)
            try:
                import json, os
                path = '/opt/bist-pattern/logs/health_status.json'
                os.makedirs('/opt/bist-pattern/logs', exist_ok=True)
                with open(path, 'w') as f:
                    json.dump(health_status, f)
            except Exception:
                pass
            # Broadcast (optional)
            try:
                from app import app as flask_app
                if hasattr(flask_app, 'broadcast_log'):
                    flask_app.broadcast_log('INFO', f"Health: {health_status['overall_status']}", 'health')
            except Exception:
                pass
            return health_status
        except Exception as e:
            logger.error(f"âŒ Internal health check hatasÄ±: {e}")
            return {'overall_status': 'error', 'error': str(e)}
    def _merge_predictions_file(self, symbol: str, out_sym: dict) -> bool:
        """`ml_bulk_predictions.json` dosyasÄ±na sembol bazlÄ± tahmini birleÅŸtirerek yazar."""
        try:
            log_dir = os.getenv('BIST_LOG_PATH', '/opt/bist-pattern/logs')
            os.makedirs(log_dir, exist_ok=True)
            fpath = os.path.join(log_dir, 'ml_bulk_predictions.json')
            import json
            data = {'timestamp': datetime.now().isoformat(), 'predictions': {}}
            if os.path.exists(fpath):
                try:
                    with open(fpath, 'r') as rf:
                        prev = json.load(rf) or {}
                        if isinstance(prev, dict):
                            data['predictions'] = prev.get('predictions') or {}
                except Exception:
                    pass
            data['predictions'][symbol] = out_sym or {}
            with open(fpath, 'w') as wf:
                json.dump(data, wf)
            return True
        except Exception as _err:
            logger.warning(f"Predictions merge error for {symbol}: {_err}")
            return False

    def run_incremental_cycle(self) -> dict:
        """Sembol bazlÄ± dÃ¶ngÃ¼: her sembol iÃ§in tek tek veri toplama â†’ analiz â†’ tahmin.

        DÄ±ÅŸ servis yÃ¼kÃ¼nÃ¼ azaltmak ve CPU/bellek kullanÄ±mÄ±nÄ± yaymak iÃ§in tam toplama yerine
        sembol bazÄ±nda ardÄ±ÅŸÄ±k Ã§alÄ±ÅŸÄ±r.
        """
        stats = {'processed': 0, 'analyzed': 0, 'predicted': 0}
        try:
            # HazÄ±rlÄ±k
            from pattern_detector import HybridPatternDetector
            det = HybridPatternDetector()
            col = None
            try:
                col = get_data_collector()
            except Exception:
                col = None
            # Semboller
            from app import app as flask_app
            with flask_app.app_context():
                from models import Stock
                symbols = [s.symbol for s in Stock.query.filter_by(is_active=True).all()]
            # ML sistemleri
            basic = None
            try:
                from ml_prediction_system import get_ml_prediction_system
                basic = get_ml_prediction_system()
            except Exception:
                basic = None
            use_enhanced = str(os.getenv('ENABLE_ENHANCED_ML', 'false')).lower() in ('1','true','yes')
            enhanced = None
            if use_enhanced:
                try:
                    from enhanced_ml_system import get_enhanced_ml_system
                    enhanced = get_enhanced_ml_system()
                except Exception:
                    enhanced = None

            # Uyku ayarlarÄ±
            try:
                symbol_sleep = float(os.getenv('SYMBOL_SLEEP_SECONDS', '0.3'))
            except Exception:
                symbol_sleep = 0.3

            for sym in symbols:
                try:
                    # 1) Veri gÃ¼ncelle (hafif)
                    if col is not None:
                        try:
                            col.update_single_stock(sym, days=7)
                        except Exception:
                            pass
                    # 2) Analiz
                    try:
                        det.analyze_stock(sym)
                        stats['analyzed'] += 1
                    except Exception:
                        pass
                    # 3) Tahmin ve dosyaya yaz (birleÅŸtirerek)
                    out_sym: dict = {}
                    df = self._get_stock_dataframe(sym)
                    if df is not None and len(df) >= 50:
                        # Basic
                        if basic is not None:
                            try:
                                preds = basic.predict_prices(sym, df, None) or {}
                                out_sym['basic'] = preds
                            except Exception:
                                pass
                        # Enhanced
                        if enhanced is not None and len(df) >= 200:
                            try:
                                enhanced.train_enhanced_models(sym, df)
                                ep = enhanced.predict_enhanced(sym, df) or {}
                                out_sym['enhanced'] = ep
                            except Exception:
                                pass
                    if out_sym:
                        if self._merge_predictions_file(sym, out_sym):
                            stats['predicted'] += 1
                    stats['processed'] += 1
                except Exception:
                    pass
                # DÄ±ÅŸ servislere nazik ol
                try:
                    time.sleep(symbol_sleep)
                except Exception:
                    pass
            return stats
        except Exception as e:
            logger.error(f"âŒ Incremental cycle error: {e}")
            return stats
    def setup_schedule(self):
        """Zamanlama ayarlarÄ±"""
        try:
            logger.info("â° Scheduled tasks ayarlanÄ±yor...")
            
            # Clear existing jobs first
            schedule.clear()
            
            # Only configure when PIPELINE_MODE explicitly SCHEDULED
            if os.getenv('PIPELINE_MODE', 'CONTINUOUS_FULL').upper() != 'SCHEDULED':
                logger.info("ğŸ›‘ PIPELINE_MODE != SCHEDULED â†’ schedule jobs are skipped (continuous mode)")
                return True
            
            # MINIMAL TEST: NO JOBS AT ALL (test schedule library itself)
            # def simple_heartbeat():
            #     logger.info("ğŸ’“ Scheduler heartbeat - thread alive")
            #     return True
            
            # NO JOBS - Pure schedule.run_pending() test
            # schedule.every(2).minutes.do(simple_heartbeat).tag('heartbeat')
            
            # STEP 1: En basit job'dan baÅŸla - daily_status_report
            schedule.every().day.at("08:00").do(self.daily_status_report).tag('daily')
            
            # DiÄŸer complex job'lar geÃ§ici olarak devre dÄ±ÅŸÄ±
            # schedule.every().day.at("06:00").do(self.daily_data_collection).tag('daily')
            # schedule.every().day.at("07:00").do(self.auto_model_retraining).tag('daily') 
            # schedule.every().monday.at("05:00").do(self.weekly_full_collection).tag('weekly')
            # schedule.every(6).hours.do(self.system_health_check).tag('health')
            
            # Ä°Ã§ pipeline iÅŸleri (dashboard kontrollÃ¼)
            # 30 dakikada bir: Ã¶ncelikli toplama â†’ AI analizi
            schedule.every(30).minutes.do(self.run_priority_pipeline).tag('priority_pipeline')
            # GÃ¼n iÃ§inde 3 kez + pazar gecesi: tam toplama â†’ AI analizi
            schedule.every().day.at("09:30").do(self.run_full_pipeline).tag('full_pipeline')
            schedule.every().day.at("12:00").do(self.run_full_pipeline).tag('full_pipeline')
            schedule.every().day.at("18:00").do(self.run_full_pipeline).tag('full_pipeline')
            schedule.every().sunday.at("02:00").do(self.run_full_pipeline).tag('weekly_full')
            # 15 dakikada bir health check
            schedule.every(15).minutes.do(self.system_health_check).tag('health')

            # Test iÃ§in - her 2 dakikada bir health check (opsiyonel)
            if os.getenv('BIST_DEBUG', '').lower() == 'true':
                schedule.every(2).minutes.do(self.system_health_check).tag('debug')
                logger.info("ğŸ”§ Debug mode: Health check every 2 minutes")
            
            job_count = len(schedule.jobs)
            logger.info(f"âœ… Scheduled tasks kuruldu ({job_count} job):")
            logger.info("  ğŸ”„ Her 30 dk - Ã–ncelikli toplama + AI analiz")
            logger.info("  ğŸ“… 09:30/12:00/18:00 - Tam toplama + AI analiz")
            logger.info("  ğŸ•‘ Pazar 02:00 - HaftalÄ±k tam toplama + AI analiz")
            logger.info("  ğŸ” Her 15 dk - Health check")
            
            return job_count > 0
            
        except Exception as e:
            logger.error(f"âŒ Schedule kurulum hatasÄ±: {e}")
            return False
    
    def start_scheduler(self):
        """Scheduler'Ä± baÅŸlat"""
        try:
            if self.is_running:
                logger.warning("âš ï¸ Scheduler zaten Ã§alÄ±ÅŸÄ±yor")
                return False
            
            logger.info("ğŸš€ Automated Data Pipeline baÅŸlatÄ±lÄ±yor...")
            
            # Tek mod: CONTINUOUS_FULL (sadeleÅŸtirildi)
            mode = 'CONTINUOUS_FULL'

            # YardÄ±mcÄ±: pipeline history'ye kayÄ±t ekle
            def _append_pipeline_history(phase: str, state: str, details: dict = None):
                try:
                    log_dir = '/opt/bist-pattern/logs'
                    os.makedirs(log_dir, exist_ok=True)
                    status_file = os.path.join(log_dir, 'pipeline_status.json')
                    payload = {'history': []}
                    try:
                        if os.path.exists(status_file):
                            with open(status_file, 'r') as f:
                                payload = json.load(f) or {'history': []}
                    except Exception:
                        payload = {'history': []}
                    entry = {
                        'phase': phase,
                        'state': state,
                        'timestamp': datetime.now().isoformat(),
                        'details': details or {}
                    }
                    payload.setdefault('history', []).append(entry)
                    # Keep last 200
                    payload['history'] = payload['history'][-200:]
                    with open(status_file, 'w') as f:
                        json.dump(payload, f)
                except Exception as _hist_err:
                    logger.warning(f"Pipeline history write failed: {_hist_err}")

            # Temizlik: Ã–nce mevcut schedule iÅŸlerini temizle (mode ne olursa olsun)
            try:
                schedule.clear()
            except Exception:
                pass

            self.is_running = True
            self.user_stopped = False
            self.last_activity_ts = time.time()

            if mode == 'CONTINUOUS_FULL':
                logger.info("ğŸ” Mode: CONTINUOUS_FULL - Incremental (sembol bazlÄ±) dÃ¶ngÃ¼")

                def run_continuous_full_loop():
                    try:
                        loop_idx = 0
                        while self.is_running:
                            loop_idx += 1
                            # heartbeat: etkinlik gÃ¼ncelle
                            self.last_activity_ts = time.time()
                            try:
                                from app import app as flask_app
                                if hasattr(flask_app, 'broadcast_log'):
                                    flask_app.broadcast_log('INFO', f'Cycle {loop_idx}: Incremental cycle starting', 'collector')
                            except Exception:
                                pass

                            # Incremental: sembol bazlÄ± toplama â†’ analiz â†’ tahmin
                            _append_pipeline_history('incremental_cycle', 'start', {'cycle': loop_idx})
                            try:
                                inc = self.run_incremental_cycle()
                                _append_pipeline_history('incremental_cycle', 'end', {'cycle': loop_idx, **(inc or {})})
                                self.last_activity_ts = time.time()
                            except Exception as e:
                                _append_pipeline_history('incremental_cycle', 'error', {'error': str(e)})
                                logger.error(f"Incremental cycle error: {e}")

                            # 4) Bekle (5 dakika)
                            try:
                                from app import app as flask_app
                                if hasattr(flask_app, 'broadcast_log'):
                                    flask_app.broadcast_log('INFO', 'Sleeping 300s before next cycle', 'scheduler')
                            except Exception:
                                pass
                            for _ in range(300):
                                if not self.is_running:
                                    break
                                # heartbeat: uykuda da etkinlik gÃ¼ncelle (panelde idle zannedilmesin)
                                if _ % 30 == 0:
                                    self.last_activity_ts = time.time()
                                time.sleep(1)
                        logger.info("â¹ï¸ Continuous loop stopped")
                    except Exception as e:
                        logger.error(f"âŒ Continuous loop critical error: {e}")
                        # is_running bayraÄŸÄ±nÄ± kapatmayalÄ±m ki watchdog devreye girebilsin
                        # BÃ¶ylece UI "STOPPED" gÃ¶stermeden otomatik restart yapÄ±lÄ±r
                        try:
                            from app import app as flask_app
                            if hasattr(flask_app, 'broadcast_log'):
                                flask_app.broadcast_log('ERROR', f'Continuous loop crashed: {e}', 'scheduler')
                        except Exception:
                            pass

                self.scheduler_thread = threading.Thread(target=run_continuous_full_loop, daemon=False)
                self.scheduler_thread.start()
                logger.info("âœ… Continuous automation loop started")
                # Idle watchdog (tek sefer baÅŸlatÄ±lacak)
                def _idle_monitor():
                    max_idle = int(float(os.getenv('MAX_IDLE_SECONDS', '900')))  # 15 dk varsayÄ±lan
                    while True:
                        try:
                            now = time.time()
                            if self.is_running and (now - float(self.last_activity_ts or 0.0)) > max_idle:
                                if not self.user_stopped:
                                    logger.warning(f"â° Idle watchdog: {int(now - self.last_activity_ts)}s hareketsizlik. Restart ediliyor...")
                                    try:
                                        self.is_running = False
                                        try:
                                            schedule.clear()
                                        except Exception:
                                            pass
                                        time.sleep(0.2)
                                        self.start_scheduler()
                                    except Exception as _idle_err:
                                        logger.error(f"Idle watchdog restart failed: {_idle_err}")
                        except Exception:
                            pass
                        time.sleep(30)
                if self._idle_watchdog_thread is None or not self._idle_watchdog_thread.is_alive():
                    self._idle_watchdog_thread = threading.Thread(target=_idle_monitor, daemon=True)
                    self._idle_watchdog_thread.start()
                return True

            # Scheduled mod kaldÄ±rÄ±ldÄ±
            logger.info("â„¹ï¸ Scheduled mode is removed; running continuous loop only")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Scheduler baÅŸlatma hatasÄ±: {e}")
            self.is_running = False
            return False
    
    def stop_scheduler(self):
        """Scheduler'Ä± durdur"""
        try:
            if not self.is_running:
                logger.warning("âš ï¸ Scheduler zaten durmuÅŸ")
                return True
            
            logger.info("ğŸ›‘ Automated Data Pipeline durduruluyor...")
            
            # History: clear file on explicit stop (requested behavior)
            try:
                log_dir = '/opt/bist-pattern/logs'
                os.makedirs(log_dir, exist_ok=True)
                status_file = os.path.join(log_dir, 'pipeline_status.json')
                with open(status_file, 'w') as f:
                    json.dump({'history': []}, f)
            except Exception:
                pass

            self.is_running = False
            self.user_stopped = True
            schedule.clear()
            
            # Thread'in bitmesini bekle
            if self.scheduler_thread and self.scheduler_thread.is_alive():
                self.scheduler_thread.join(timeout=5)
            
            logger.info("âœ… Automated Data Pipeline durduruldu")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Scheduler durdurma hatasÄ±: {e}")
            return False
    
    def get_scheduler_status(self):
        """Scheduler durumu"""
        try:
            # Thread health check
            thread_alive = self.scheduler_thread.is_alive() if self.scheduler_thread else False
            
            # Debug: Thread death detection (NO auto-restart)
            if self.is_running and not thread_alive:
                logger.error("âŒ CRITICAL: Scheduler thread Ã¶ldÃ¼! Root cause araÅŸtÄ±rÄ±lmalÄ±.")
                logger.error("ğŸ” Thread alive: False, is_running: True - Bu durumun sebebi bulunmalÄ±")
                # Otomatik yeniden baÅŸlatma (watchdog) - varsayÄ±lan aÃ§Ä±k
                try:
                    enabled = str(os.getenv('ENABLE_SCHEDULER_WATCHDOG', 'true')).lower() in ('1', 'true', 'yes')
                except Exception:
                    enabled = True
                if enabled:
                    try:
                        now = time.time()
                        # 30 sn'den sÄ±k restart etme
                        if now - float(self.last_watchdog_restart_ts or 0.0) > 30.0:
                            logger.warning("ğŸ› ï¸ Watchdog: Scheduler thread dead, restarting...")
                            self.last_watchdog_restart_ts = now
                            def _do_restart():
                                try:
                                    # GÃ¼venli sÄ±fÄ±rlama ve yeniden baÅŸlat
                                    self.is_running = False
                                    try:
                                        schedule.clear()
                                    except Exception:
                                        pass
                                    # KÄ±sa gecikme ile yeniden baÅŸlat
                                    time.sleep(0.2)
                                    self.start_scheduler()
                                except Exception as e:
                                    logger.error(f"Watchdog restart failed: {e}")
                            threading.Thread(target=_do_restart, daemon=True).start()
                    except Exception as _wd_err:
                        logger.error(f"Watchdog error: {_wd_err}")
            
            # schedule.jobs iÃ§i boÅŸ olsa bile (pure loop modunda) UI'da 1 iÅŸ gÃ¶sterelim
            try:
                scheduled_jobs_count = len(schedule.jobs)
            except Exception:
                scheduled_jobs_count = 0
            if self.is_running and scheduled_jobs_count == 0:
                scheduled_jobs_count = 1

            status = {
                'is_running': self.is_running,
                'thread_alive': thread_alive,
                'scheduled_jobs': scheduled_jobs_count,
                'last_run_stats': self.last_run_stats,
                'next_runs': []
            }
            
            # Sonraki Ã§alÄ±ÅŸma zamanlarÄ±
            for job in schedule.jobs:
                try:
                    next_run = job.next_run
                    status['next_runs'].append({
                        'job': str(job.job_func.__name__),
                        'next_run': next_run.isoformat() if next_run else None
                    })
                except:
                    pass
            
            return status
            
        except Exception as e:
            logger.error(f"âŒ Status alma hatasÄ±: {e}")
            return {'error': str(e)}
    
    def run_manual_task(self, task_name):
        """Manuel gÃ¶rev Ã§alÄ±ÅŸtÄ±rma"""
        try:
            logger.info(f"ğŸ”§ Manuel gÃ¶rev Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor: {task_name}")
            
            # Available tasks (some migrated to daemon)
            task_map = {
                'data_collection': self.daily_data_collection,
                'status_report': self.daily_status_report,
                'weekly_collection': self.weekly_full_collection,
                'bulk_predictions': self.run_bulk_predictions_all,
            }
            
            # Migrated to daemon tasks 
            migrated_tasks = ['model_retraining', 'health_check']
            if task_name in migrated_tasks:
                logger.warning(f"âš ï¸ {task_name} migrated to scheduler_daemon.py")
                return {"status": "migrated", "message": f"{task_name} is now handled by daemon"}
            
            if task_name not in task_map:
                logger.error(f"âŒ Bilinmeyen gÃ¶rev: {task_name}")
                return False
            
            result = task_map[task_name]()
            logger.info(f"âœ… Manuel gÃ¶rev tamamlandÄ±: {task_name}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Manuel gÃ¶rev hatasÄ±: {e}")
            return False

    def run_priority_pipeline(self):
        """Ã–ncelikli toplama â†’ AI analizi"""
        try:
            from advanced_collector import AdvancedBISTCollector
            from pattern_detector import HybridPatternDetector
            logger.info("ğŸš€ Ã–ncelikli pipeline baÅŸlÄ±yor: veri toplama")
            try:
                from app import app as flask_app
                if hasattr(flask_app, 'broadcast_log'):
                    flask_app.broadcast_log('INFO', 'ğŸ”„ Ã–ncelikli veri toplama baÅŸlÄ±yor', 'collector')
            except Exception:
                pass
            collector = AdvancedBISTCollector()
            col_res = collector.collect_priority_stocks()
            logger.info(f"âœ… Ã–ncelikli toplama bitti: {col_res}")
            try:
                from app import app as flask_app
                if hasattr(flask_app, 'broadcast_log'):
                    flask_app.broadcast_log('SUCCESS', f"âœ… Ã–ncelikli toplama bitti: {col_res}", 'collector')
            except Exception:
                pass
            # AI analizi
            logger.info("ğŸ§  AI analizi baÅŸlÄ±yor (Ã¶ncelikli)")
            det = HybridPatternDetector()
            analyzed = 0
            try:
                from app import app as flask_app
                with flask_app.app_context():
                    from models import Stock
                    # Ã–ncelikli semboller veya aktiflerden ilk 100
                    priority = getattr(__import__('config').config['default'], 'PRIORITY_SYMBOLS', [])
                    symbols = priority or [s.symbol for s in Stock.query.filter_by(is_active=True).limit(100).all()]
            except Exception:
                symbols = []
            for sym in symbols[:100]:
                try:
                    det.analyze_stock(sym)
                    analyzed += 1
                except Exception:
                    continue
            logger.info(f"ğŸ¯ AI analizi tamamlandÄ±: {analyzed} hisse")
            try:
                from app import app as flask_app
                if hasattr(flask_app, 'broadcast_log'):
                    flask_app.broadcast_log('SUCCESS', f"ğŸ¯ AI analizi tamamlandÄ±: {analyzed} hisse", 'ai_analysis')
            except Exception:
                pass
            return True
        except Exception as e:
            logger.error(f"âŒ Ã–ncelikli pipeline hatasÄ±: {e}")
            return False

    def run_full_pipeline(self):
        """Tam toplama â†’ AI analizi"""
        try:
            from advanced_collector import AdvancedBISTCollector
            from pattern_detector import HybridPatternDetector
            logger.info("ğŸš€ Tam pipeline baÅŸlÄ±yor: veri toplama")
            try:
                from app import app as flask_app
                if hasattr(flask_app, 'broadcast_log'):
                    flask_app.broadcast_log('INFO', 'ğŸ“Š Tam veri toplama baÅŸlÄ±yor', 'collector')
            except Exception:
                pass
            collector = AdvancedBISTCollector()
            res = collector.collect_all_stocks_parallel()
            logger.info(f"âœ… Tam toplama bitti: {res}")
            try:
                from app import app as flask_app
                if hasattr(flask_app, 'broadcast_log'):
                    flask_app.broadcast_log('SUCCESS', f"âœ… Tam toplama bitti: {res}", 'collector')
            except Exception:
                pass
            # AI analizi
            logger.info("ğŸ§  AI analizi baÅŸlÄ±yor (tam)")
            det = HybridPatternDetector()
            analyzed = 0
            try:
                from app import app as flask_app
                with flask_app.app_context():
                    from models import Stock
                    symbols = [s.symbol for s in Stock.query.filter_by(is_active=True).all()]
            except Exception:
                symbols = []
            for sym in symbols[:600]:
                try:
                    det.analyze_stock(sym)
                    analyzed += 1
                except Exception:
                    continue
            logger.info(f"ğŸ¯ AI analizi tamamlandÄ±: {analyzed} hisse")
            try:
                from app import app as flask_app
                if hasattr(flask_app, 'broadcast_log'):
                    flask_app.broadcast_log('SUCCESS', f"ğŸ¯ AI analizi tamamlandÄ±: {analyzed} hisse", 'ai_analysis')
            except Exception:
                pass
            return True
        except Exception as e:
            logger.error(f"âŒ Tam pipeline hatasÄ±: {e}")
            return False
# Global singleton instance
_automated_pipeline = None

def get_automated_pipeline():
    """Automated Pipeline singleton'Ä±nÄ± dÃ¶ndÃ¼r"""
    global _automated_pipeline
    if _automated_pipeline is None:
        _automated_pipeline = AutomatedDataPipeline()
    return _automated_pipeline

if __name__ == "__main__":
    # Test run
    pipeline = get_automated_pipeline()
    
    print("ğŸš€ Automated Data Pipeline Test baÅŸlatÄ±lÄ±yor...")
    
    # Manuel gÃ¶rev testleri
    # Health check migrated to daemon; skip direct call here to avoid missing method errors
    print("\nğŸ“Š Health Check Test: (skipped - handled by scheduler_daemon.py)")
    
    print("\nğŸ“ˆ Data Collection Test:")
    data_result = pipeline.daily_data_collection()
    print(f"Result: {data_result}")
    
    print("\nğŸ“‹ Status Report Test:")
    report = pipeline.daily_status_report()
    if report:
        print("Report generated successfully")
    
    print("\nâ° Scheduler Start Test:")
    if pipeline.start_scheduler():
        print("âœ… Scheduler baÅŸlatÄ±ldÄ±")
        
        # 30 saniye bekle
        print("â³ 30 saniye test...")
        time.sleep(30)
        
        # Status kontrol
        status = pipeline.get_scheduler_status()
        print(f"ğŸ“Š Scheduler Status: {status}")
        
        # Durdur
        pipeline.stop_scheduler()
        print("ğŸ›‘ Scheduler durduruldu")
    else:
        print("âŒ Scheduler baÅŸlatÄ±lamadÄ±")
    
    print("\nğŸ¯ Test tamamlandÄ±!")
