## HPO Pipeline DetaylÄ± DokÃ¼mantasyon

Bu dokÃ¼man, **HPO (Hyperparameter Optimization) + Training** sÃ¼recini uÃ§tan uca, kod referanslarÄ±yla birlikte aÃ§Ä±klar.  
AmaÃ§, sÃ¼reci hem **teknik/mantÄ±ksal** aÃ§Ä±dan hem de **iÅŸ/kural** bakÄ±ÅŸ aÃ§Ä±sÄ±ndan tamamen ÅŸeffaf ve denetlenebilir hale getirmektir.

---

### 1. Genel Mimari Ã–zeti

- **AmaÃ§**: Her sembolâ€“ufuk (`symbol_horizon`) Ã§ifti iÃ§in:
  - Optuna ile **feature flags + feature iÃ§ parametreleri + model hyperparameter** optimizasyonu (HPO),
  - AynÄ± parametrelerle **WFV (walkâ€‘forward validation) temelli eÄŸitim**,
  - SonuÃ§larÄ±n **DirHit, nRMSE, skor** ve mask filtreleri ile tutarlÄ± ÅŸekilde kaydedilmesi.
- **Temel bileÅŸenler**:
  - **HPO Objective**: `scripts/optuna_hpo_with_feature_flags.py`
  - **Orkestrasyon (HPO + Training + Cycle)**: `scripts/continuous_hpo_training_pipeline.py`
  - **Study dosyalarÄ± (Optuna)**: `hpo_studies/hpo_with_features_{SYMBOL}_h{H}_c{CYCLE}.db`
  - **HPO JSON Ã§Ä±ktÄ± dosyalarÄ±**: `results/optuna_pilot_features_on_h{H}_c{CYCLE}_*.json`
  - **Durum dosyasÄ± (state)**: `results/continuous_hpo_state.json`
  - **Filtreli JSON tekrar Ã¼retimi**: `scripts/recreate_all_json_from_study_with_filter.py`
  - **DÃ¼ÅŸÃ¼k destek fallback best params**: `scripts/find_fallback_best_params.py`
  - **Ä°lerleme gÃ¶rÃ¼nÃ¼mÃ¼**: `scripts/show_hpo_progress.py`

---

### 2. Ana Dosyalar ve Roller

- **`scripts/optuna_hpo_with_feature_flags.py`**  
  - Optuna **objective** fonksiyonunu tanÄ±mlar (`objective`), trial bazÄ±nda:
    - Feature flag kombinasyonlarÄ±nÄ±,
    - Feature iÃ§ parametrelerini,
    - Model hyperparametrelerini,
    - Model seÃ§imini (`model_choice`) optimize eder.
  - Her trial iÃ§in **Ã§oklu WFV split** Ã¼zerinde DirHit ve nRMSE hesaplar.
  - SonuÃ§larÄ± **`trial.user_attrs`** iÃ§ine yazar: `avg_dirhit`, `avg_nrmse`, `symbol_metrics`, `features_enabled`, `feature_params` vb.

- **`scripts/continuous_hpo_training_pipeline.py`**  
  - `ContinuousHPOPipeline` sÄ±nÄ±fÄ±:
    - TÃ¼m sembolâ€“ufuk iÅŸleri iÃ§in **HPO + Training** sÃ¼recini yÃ¶netir.
    - `continuous_hpo_state.json` durum dosyasÄ±nÄ± okur/yazar.
    - **Cycle yÃ¶netimi** yapar (Cycle 1, 2, 3...).
    - Paralel iÅŸlem (ProcessPoolExecutor) ile sembolleri horizon bazlÄ± iÅŸler.
    - HPO sonucunu JSON + study dosyalarÄ±ndan **filtre kurallarÄ±yla birlikte** seÃ§er.
    - EÄŸitim sonrasÄ± WFV ve online DirHit Ã¶lÃ§Ã¼mlerini hesaplar.

- **`scripts/recreate_all_json_from_study_with_filter.py`**  
  - Study dosyalarÄ±ndan, belirtilen filtre ile (`min_mask_count`, `min_mask_pct`, `min_valid_splits`) **JSON yeniden Ã¼retir**.
  - Ã–nce 5/2.5, gerekirse fallback 0/0.0 filtresi ile Ã§alÄ±ÅŸÄ±r.

- **`scripts/find_fallback_best_params.py`**  
  - Bir sembolâ€“ufuk iÃ§in study iÃ§indeki trialâ€™larÄ± dolaÅŸÄ±r,
  - **0/0.0 filtre** (mask filtresi yok) ile split DirHit ortalamasÄ±na gÃ¶re en iyi trialâ€™Ä± bulur,
  - DÃ¼ÅŸÃ¼k destekli semboller iÃ§in **fallback best params** saÄŸlar.

- **`scripts/show_hpo_progress.py`**  
  - `continuous_hpo_state.json` + study dosyalarÄ±ndan:
    - HPO trial sayÄ±sÄ±, en iyi trial, DirHit,
    - EÄŸitim DirHit (WFV/online) durumlarÄ±nÄ± gÃ¶sterir.
  - DirHitâ€™i **`symbol_metrics[symbol_key]['avg_dirhit']`** Ã¼zerinden **sembol spesifik** okur.

---

### 3. Optuna HPO Objective (optuna_hpo_with_feature_flags.py)

#### 3.1. Parametre UzayÄ± (Feature Flags + Feature Parametreleri + Hyperparametreler)

Objective fonksiyonu:

```288:367:scripts/optuna_hpo_with_feature_flags.py
def objective(trial: optuna.Trial, symbols, horizon: int, engine, db_url: str, study=None, max_trials: Optional[int] = None) -> float:
    """Optuna objective function - Feature flags + Hyperparameters birlikte optimize edilir."""
    # âœ… FIX: Check trial limit at the start of each trial to prevent exceeding n_trials
    # This provides a second layer of protection against race conditions in parallel execution
    # Count all trials except the current one (which is just starting)
    if study is not None and max_trials is not None:
        # Count all trials except the current one
        other_trials = [t for t in study.trials if t.number != trial.number]
        if len(other_trials) >= max_trials:
            # Skip this trial if we've already reached the limit
            raise optuna.TrialPruned(f"Trial limit reached ({len(other_trials)}/{max_trials})")
    
    ConfigManager.clear_cache()
    
    # âš¡ NEW: Feature flag'leri optimize et (12 feature - test script ile aynÄ±)
    feature_flags = {
        'enable_external_features': trial.suggest_categorical('enable_external_features', [True, False]),
        'enable_fingpt_features': trial.suggest_categorical('enable_fingpt_features', [True, False]),
        'enable_yolo_features': trial.suggest_categorical('enable_yolo_features', [True, False]),
        'enable_directional_loss': trial.suggest_categorical('enable_directional_loss', [True, False]),
        'enable_seed_bagging': trial.suggest_categorical('enable_seed_bagging', [True, False]),
        'enable_talib_patterns': trial.suggest_categorical('enable_talib_patterns', [True, False]),
        'enable_smart_ensemble': trial.suggest_categorical('enable_smart_ensemble', [True, False]),
        'enable_stacked_short': trial.suggest_categorical('enable_stacked_short', [True, False]),
        'enable_meta_stacking': trial.suggest_categorical('enable_meta_stacking', [True, False]),
        'enable_regime_detection': trial.suggest_categorical('enable_regime_detection', [True, False]),
        'enable_fingpt': trial.suggest_categorical('enable_fingpt', [True, False]),
        # ML_USE_ADAPTIVE_LEARNING: HPO'da her zaman kapalÄ± (data leakage Ã¶nleme)
    }
    ...
    # Adaptive Learning parametreleri (her zaman optimize et, Ã§Ã¼nkÃ¼ Phase 2 skip ediliyor ama Phase 1'de kullanÄ±lÄ±yor)
    horizon_key = f'{horizon}d'
    feature_params[f'ml_adaptive_k_{horizon_key}'] = trial.suggest_float(f'ml_adaptive_k_{horizon_key}', 1.0, 3.0)
    feature_params[f'ml_pattern_weight_scale_{horizon_key}'] = trial.suggest_float(f'ml_pattern_weight_scale_{horizon_key}', 0.5, 2.0)
```

Ã–zet:
- **Feature flags**: `enable_*` bayraklarÄ± ile hangi feature setâ€™lerinin aÃ§Ä±k/kapalÄ± olacaÄŸÄ± optimize edilir.
- **Feature parametreleri**: Directional loss, seed bagging, meta stacking, adaptive learning, YOLO, smart ensemble vb.
- **Model seÃ§imi**: `model_choice` ile `xgb`, `lgbm`, `cat`, `all` seÃ§enekleri; sadece ortamda mevcut modeller arasÄ±ndan.
- **Hyperparametreler**: `xgb_*`, `lgb_*`, `cat_*` ana model parametreleri.

#### 3.2. Veri Alma ve Walk-Forward Split Ãœretimi

Her trial iÃ§in her sembol Ã¼zerinde:

```580:607:scripts/optuna_hpo_with_feature_flags.py
    trial_symbol_metrics: Dict[str, Dict[str, Any]] = {}
    print(f"[hpo] Trial {trial.number}: Processing {len(symbols)} symbols: {symbols}", file=sys.stderr, flush=True)
    
    for sym in symbols:
        symbol_key = f"{sym}_{horizon}d"
        symbol_metric_entry: Dict[str, Any] = {
            'symbol': sym,
            'horizon': horizon,
            'split_metrics': []
        }
        trial_symbol_metrics[symbol_key] = symbol_metric_entry
        print(f"[hpo] Trial {trial.number}: Fetching prices for {sym}...", file=sys.stderr, flush=True)
        df = fetch_prices(engine, sym)
        ...
        # âš¡ FIX: Minimum data requirement - all horizons require 100 days
        min_required_days = 100  # All horizons require minimum 100 days
        if len(df) < min_required_days:
            print(f"[hpo] Trial {trial.number}: {sym} - len(df)={len(df)} < {min_required_days} (min required for {horizon}d), skipping", file=sys.stderr, flush=True)
            continue
        ...
        # âš¡ NEW: Generate multiple splits for walk-forward validation
        total_days = len(df)
        wfv_splits = generate_walkforward_splits(total_days, horizon, n_splits=4)
```

- Her sembol iÃ§in **en az 100 gÃ¼n veri zorunluluÄŸu** var.
- Veri yeterliyse, 4 adet **WFV split** Ã¼retiliyor (`generate_walkforward_splits`).
- Splitâ€™ler Ã¼zerinde model eÄŸitimi + tahmin yapÄ±lÄ±yor; DirHit ve nRMSE hesaplanÄ±yor.

#### 3.3. DirHit, nRMSE ve Score HesabÄ± + symbol_metrics

Split bazlÄ± metriklerden sembol bazlÄ± ve trial bazlÄ± Ã¶zetler:

```874:915:scripts/optuna_hpo_with_feature_flags.py
        # Average DirHit across all splits
        # âœ… FIX: Require at least 2 splits for reliable DirHit calculation
        # Single split DirHit is statistically unreliable
        avg_dirhit_value = None
        if len(split_dirhits) >= 2:
            avg_dirhit_value = float(np.mean(split_dirhits))
            print(
                f"[hpo] {sym} {horizon}d: Average DirHit across {len(split_dirhits)} splits: {avg_dirhit_value:.2f}% "
                f"(splits: {split_dirhits})",
                file=sys.stderr, flush=True
            )
            dirhits.append(avg_dirhit_value)
        elif len(split_dirhits) == 1:
            ...
            symbol_metric_entry['low_support_warning'] = True
        else:
            ...
            symbol_metric_entry['low_support_warning'] = True
        # Compute per-symbol nRMSE as the average across split nRMSE values
        avg_nrmse_value = None
        if split_nrmses_local:
            try:
                avg_nrmse_local = float(np.mean(split_nrmses_local))
                nrmses.append(avg_nrmse_local)
                avg_nrmse_value = avg_nrmse_local
            except Exception:
                pass
        symbol_metric_entry['avg_dirhit'] = avg_dirhit_value
        symbol_metric_entry['avg_nrmse'] = avg_nrmse_value
        symbol_metric_entry['split_count'] = len(symbol_metric_entry['split_metrics'])
        symbol_metric_entry['avg_model_metrics'] = _aggregate_model_metrics(symbol_metric_entry['split_metrics'])
```

Trial seviyesi skor ve user_attrs:

```929:947:scripts/optuna_hpo_with_feature_flags.py
    avg_dirhit = float(np.mean(dirhits))
    avg_nrmse = float(np.mean(nrmses)) if nrmses else float('inf')
    k = 6.0 if horizon in (1, 3, 7) else 4.0
    score = float(0.7 * avg_dirhit - k * (avg_nrmse if np.isfinite(avg_nrmse) else 3.0))
    print(f"[hpo] Trial {trial.number}: Average DirHit={avg_dirhit:.2f}% (from {len(dirhits)} symbols), nRMSE={avg_nrmse:.3f}, score={score:.2f}", file=sys.stderr, flush=True)
    try:
        trial.set_user_attr('avg_dirhit', avg_dirhit)
        trial.set_user_attr('avg_nrmse', avg_nrmse)
        trial.set_user_attr('model_choice', model_choice)
        # âœ… FIX: Store symbol_metrics in trial user_attrs so it can be retrieved later for best_trial_metrics
        if trial_symbol_metrics:
            ...
            trial.set_user_attr('symbol_metrics', trial_symbol_metrics)
```

**Ã–nemli noktalar:**
- **Split seviyesinde** DirHit hesaplanÄ±yor, ardÄ±ndan en az **2 split varsa** ortalamasÄ± alÄ±nÄ±yor.
- `symbol_metrics[symbol_key]['avg_dirhit']` her sembolâ€“ufuk iÃ§in **sembol spesifik** DirHitâ€™i tutuyor.
- `trial.user_attrs['avg_dirhit']` ise bir trialâ€™da kullanÄ±lan **tÃ¼m sembollerin ortalamasÄ±**.  
  - Biz **sembol bazÄ±nda** karar verirken **symbol_metrics**â€™i kullanÄ±yoruz (bu, son fixâ€™lerle garanti altÄ±na alÄ±ndÄ±).

---

### 4. Durum YÃ¶netimi ve Cycle MantÄ±ÄŸÄ± (continuous_hpo_training_pipeline.py)

#### 4.1. TaskState ve State DosyasÄ±

Her sembolâ€“ufuk Ã§ifti iÃ§in durum:

```605:621:scripts/continuous_hpo_training_pipeline.py
@dataclass
class TaskState:
    """Task state for a symbol-horizon pair"""
    symbol: str
    horizon: int
    status: str  # 'pending', 'hpo_in_progress', 'training_in_progress', 'completed', 'failed', 'skipped'
    hpo_completed_at: Optional[str] = None
    training_completed_at: Optional[str] = None
    best_params_file: Optional[str] = None
    hpo_dirhit: Optional[float] = None
    training_dirhit: Optional[float] = None  # backward compatibility (WFV)
    training_dirhit_wfv: Optional[float] = None
    training_dirhit_online: Optional[float] = None
    adaptive_dirhit: Optional[float] = None  # NEW: Adaptive learning DirHit (online DirHit with adaptive learning enabled)
    error: Optional[str] = None
    cycle: int = 0
    retry_count: int = 0  # âœ… FIX: Retry count for failed HPO tasks
```

- Durum dosyasÄ±: `STATE_FILE = Path('/opt/bist-pattern/results/continuous_hpo_state.json')`  
- Her entry keyâ€™i: `"{symbol}_{horizon}d"`.
- `status` alanÄ± HPO ve trainingâ€™in hangi aÅŸamada olduÄŸunu gÃ¶sterir.

#### 4.2. Cycle YÃ¶netimi (run_cycle)

Cycle mantÄ±ÄŸÄ±:

```3990:4038:scripts/continuous_hpo_training_pipeline.py
    def run_cycle(self):
        """Run one complete cycle
        
        âœ… NEW APPROACH: Horizon-First processing (USER REQUEST)
        - Processes ALL symbols for each horizon before moving to next
        - Phase 1: All symbols for 1d â†’ Phase 2: All symbols for 3d â†’ ...
        - Incremental value delivery: 1d ready for all symbols first!
        - MAX_WORKERS: Symbols processed in parallel within each horizon phase
        """
        # âœ… CRITICAL FIX: Only increment cycle if current cycle is complete
        self.load_state()
        current_cycle = self.cycle
        ...
        if not has_incomplete and current_cycle > 0:
            ...
            self.cycle += 1
            ...
        else:
            ...
        logger.info(f"ğŸ”„ Starting cycle {self.cycle}")
        ...
        # âœ… FIX: Clean up old cycle files before starting new cycle
        if self.cycle > 1:  # Don't clean on first cycle
            logger.info("ğŸ§¹ Cleaning up old cycle files (keeping only current cycle)...")
            self.cleanup_old_cycle_files(keep_cycles=1)
        ...
        # âœ… NEW: Horizon-First processing
        for horizon in HORIZON_ORDER:
            ...
            symbols_all = self.get_active_symbols()
            ...
            executor = ProcessPoolExecutor(max_workers=MAX_WORKERS)
            ...
            future = executor.submit(process_task_standalone, symbol, horizon, self.cycle)
```

Ã–zet:
- **Cycle**, tÃ¼m sembolâ€“ufuk iÅŸleri bittiÄŸinde artar (tam cycle tamamlanmadan artmaz).
- Her cycle baÅŸÄ±nda **eski cycleâ€™a ait study/JSON dosyalarÄ± temizlenir** (current cycle hariÃ§).
- Cycle iÃ§inde **horizonâ€‘first**:
  - Ã–nce tÃ¼m semboller iÃ§in 1d,
  - sonra tÃ¼m semboller iÃ§in 3d,
  - vb.  
  BÃ¶ylece 1d ufku tÃ¼m semboller iÃ§in Ã¶nce hazÄ±r olur (iÅŸ aÃ§Ä±sÄ±ndan mantÄ±klÄ±).

---

### 5. HPOâ€™nin Pipeline Ä°Ã§inde Ã‡alÄ±ÅŸtÄ±rÄ±lmasÄ± (run_hpo)

`ContinuousHPOPipeline.run_hpo` HPO scriptâ€™ini subprocess ile Ã§aÄŸÄ±rÄ±r:

```1180:1215:scripts/continuous_hpo_training_pipeline.py
    def run_hpo(self, symbol: str, horizon: int) -> Optional[Dict]:
        """Run HPO for a symbol-horizon pair"""
        try:
            logger.info(f"ğŸ”¬ Starting HPO for {symbol} {horizon}d...")
            ...
            hpo_script = Path('/opt/bist-pattern/scripts/optuna_hpo_with_feature_flags.py')
            ...
            dry_run_trials = int(os.environ.get('DRY_RUN_TRIALS', '0'))
            trials_to_use = dry_run_trials if dry_run_trials > 0 else HPO_TRIALS
            timeout_to_use = 3600 if dry_run_trials > 0 else 900000
            ...
            cmd = [
                sys.executable,
                str(hpo_script),
                '--symbols', symbol,
                '--horizon', str(horizon),
                '--trials', str(trials_to_use),
                '--timeout', str(timeout_to_use)
            ]
            ...
            env = os.environ.copy()
            ...
            env['HPO_CYCLE'] = str(self.cycle)
```

Slot & CPU affinity, log dosyalarÄ±:

```1225:1270:scripts/continuous_hpo_training_pipeline.py
            # âœ… Acquire global HPO slot (limits cross-process concurrency)
            slot_context = HPOSlotContext()
            ...
            numa_node, cpu_list = _get_numa_node_and_cpus()
            numa_cmd, _, _ = _build_numa_cmd(cmd, numa_node, cpu_list)
            ...
            hpo_log_dir = Path('/opt/bist-pattern/logs/hpo_outputs')
            ...
            stdout_file = hpo_log_dir / f"{symbol}_{horizon}d_stdout.log"
            stderr_file = hpo_log_dir / f"{symbol}_{horizon}d_stderr.log"
            ...
            process = subprocess.Popen(
                numa_cmd,
                stdout=stdout_f,
                stderr=stderr_f,
                text=True,
                cwd='/opt/bist-pattern',
                env=env,
                start_new_session=True,
                preexec_fn=lambda: os.nice(-5) if hasattr(os, 'nice') else None
            )
```

HPO tamamlandÄ±ktan sonra:
- Ã‡Ä±kÄ±ÅŸ kodu kontrol ediliyor.
- `results/optuna_pilot_features_on_h{H}_c{CYCLE}_*.json` iÃ§inden ilgili HPO sonucu seÃ§iliyor.
- EÄŸer JSON yoksa / bozuksa, **study dosyasÄ±ndan recovery** yapÄ±lmaya Ã§alÄ±ÅŸÄ±lÄ±yor.
- Son olarak, seÃ§ilen JSONâ€™dan:
  - `best_params`,
  - `best_dirhit`,
  - `features_enabled`,
  - `feature_params` vb. okunup `run_hpo` return deÄŸeri olarak dÃ¶nÃ¼yor.

JSON seÃ§im ve filtreleme mantÄ±ÄŸÄ±:

```1596:1719:scripts/continuous_hpo_training_pipeline.py
            # âœ… CRITICAL FIX: Check recent files for our symbol with timestamp validation
            ...
            valid_json_candidates = []
            
            for json_file in json_files[:50]:
                ...
                with open(json_file, 'r') as f:
                    hpo_data = json.load(f)
                ...
                # HPO tamamlanmÄ±ÅŸ mÄ±?
                n_trials = hpo_data.get('n_trials', 0)
                if not isinstance(n_trials, int) or n_trials < 10:
                    ...
                    continue
                ...
                best_dirhit = hpo_data.get('best_dirhit')
                candidate_best_score = hpo_data.get('best_value', 0)
                ...
                # LOW SUPPORT kontrolÃ¼ (mask_count, mask_pct)
                best_trial_metrics = hpo_data.get('best_trial_metrics', {})
                symbol_key_check = f"{symbol}_{horizon}d"
                if symbol_key_check in best_trial_metrics:
                    symbol_metrics = best_trial_metrics[symbol_key_check]
                    split_metrics = symbol_metrics.get('split_metrics', [])
                    ...
                    total_mask_count = sum(s.get('mask_count', 0) for s in split_metrics)
                    mask_pcts = [s.get('mask_pct', 0.0) for s in split_metrics if s.get('mask_pct') is not None]
                    avg_mask_pct = np.mean(mask_pcts) if mask_pcts else 0.0
                    ...
                    _min_mc = int(os.getenv('HPO_MIN_MASK_COUNT', '0'))
                    _min_mp = float(os.getenv('HPO_MIN_MASK_PCT', '0.0'))
                    if total_mask_count < _min_mc or avg_mask_pct < _min_mp:
                        has_low_support = True
                        ...
                valid_json_candidates.append({...})
```

Ve aday seÃ§imi:

```1769:1815:scripts/continuous_hpo_training_pipeline.py
            # âœ… FIX: Select the best JSON from all valid candidates
            # âœ… CRITICAL FIX: Prioritize candidates WITHOUT LOW SUPPORT
            # Priority: 1) No LOW SUPPORT, 2) Highest DirHit, 3) Highest best_value, 4) Most recent
            if valid_json_candidates:
                # First, separate candidates by LOW SUPPORT status
                candidates_with_support = [c for c in valid_json_candidates if not c.get('has_low_support', False)]
                candidates_low_support = [c for c in valid_json_candidates if c.get('has_low_support', False)]
                ...
                if candidates_with_support:
                    candidates_with_support.sort(
                        key=lambda x: (
                            x['best_dirhit'] if x['best_dirhit'] is not None else -1,
                            x['best_value'],
                            x['json_mtime']
                        ),
                        reverse=True
                    )
                    best_candidate = candidates_with_support[0]
                    ...
                else:
                    # âš ï¸ FALLBACK: No candidates with sufficient support, but we still want to train the model
                    ...
                    candidates_low_support.sort(...)
                    best_candidate = candidates_low_support[0]
```

**SonuÃ§**:  
HPO sonucunu seÃ§erken:
- Ã–nce **5/2.5 filtresine gÃ¶re yeterli destekli** (mask_count, mask_pct) adaylar,
- Sonra dÃ¼ÅŸÃ¼k destekli adaylar (fallback),
- Ä°Ã§lerinde de Ã¶nce **DirHit**, sonra **score (best_value)**, sonra **timestamp** kriterleri kullanÄ±lÄ±r.

---

### 6. Training SÃ¼reci (run_training + _evaluate_training_dirhits)

#### 6.1. run_training: Ortam DeÄŸiÅŸkenleri ve Feature BayraklarÄ±

```3071:3095:scripts/continuous_hpo_training_pipeline.py
    def run_training(self, symbol: str, horizon: int, best_params: Dict, hpo_result: Optional[Dict] = None) -> Optional[Dict[str, Optional[float]]]:
        """Run training with best params for a symbol-horizon pair
        ...
        """
        try:
            logger.info(f"ğŸ¯ Starting training for {symbol} {horizon}d with best params...")
            
            # Set parameters as environment variables
            from scripts.train_completed_hpo_with_best_params import set_hpo_params_as_env
            set_hpo_params_as_env(best_params, horizon)
            
            # âœ… CRITICAL FIX: Set feature flags from hpo_result (if available)
            # hpo_result contains 'features_enabled' dict with feature flags from best trial
            if hpo_result and 'features_enabled' in hpo_result:
                features_enabled = hpo_result['features_enabled']
                for key, value in features_enabled.items():
                    os.environ[key] = str(value)
                logger.info(f"ğŸ”§ {symbol} {horizon}d: Feature flags set from hpo_result: {len(features_enabled)} flags")
                ...
```

ArdÄ±ndan:

```3145:3213:scripts/continuous_hpo_training_pipeline.py
            # Set horizon
            os.environ['ML_HORIZONS'] = str(horizon)
            
            # âœ… UPDATED: Set feature flags from HPO best_params (features_enabled dict)
            # Use HPO-optimized feature flags, but always enable adaptive learning for training
            features_enabled = best_params.get('features_enabled', {})
            if features_enabled:
                # Set feature flags from HPO results
                os.environ['ENABLE_EXTERNAL_FEATURES'] = features_enabled.get('ENABLE_EXTERNAL_FEATURES', '1')
                ...
                os.environ['ENABLE_CATBOOST'] = features_enabled.get('ENABLE_CATBOOST', '0')
                ...
            else:
                # Fallback: Enable all features if features_enabled not found (backward compatibility)
                ...
            # âœ… HÄ°BRÄ°T YAKLAÅIM: Training'de adaptive learning KAPALI (HPO ile tutarlÄ±lÄ±k)
            os.environ['ML_USE_ADAPTIVE_LEARNING'] = '0'
            os.environ['ML_SKIP_ADAPTIVE_PHASE2'] = '1'
```

**Ã–zet:**
- HPO JSONâ€™dan gelen **best_params + features_enabled + feature_params**, hem:
  - `set_hpo_params_as_env` ile,
  - hem de doÄŸrudan `os.environ` Ã¼zerinden
  eÄŸitim sÄ±rasÄ±nda **birebir aynen** kullanÄ±lÄ±yor.
- Adaptive learning **HPO ile aynÄ± ÅŸekilde kapalÄ±** tutuluyor; incremental Ã¶ÄŸrenme etkisi **cycleâ€™lar** Ã¼zerinden saÄŸlanÄ±yor.

#### 6.2. EÄŸitim SonrasÄ± DirHit HesabÄ± (_evaluate_training_dirhits)

WFV evaluation:

```1955:1982:scripts/continuous_hpo_training_pipeline.py
    def _evaluate_training_dirhits(self, symbol: str, horizon: int, df: pd.DataFrame, best_params: Optional[Dict] = None, hpo_result: Optional[Dict] = None) -> Dict[str, Optional[float]]:
        """Evaluate DirHit after training using two modes:
        - wfv: adaptive OFF (no leakage)
        - online: adaptive OFF (HPO ile tutarlÄ±lÄ±k - hibrit yaklaÅŸÄ±m)
        """
        import os
        results: Dict[str, Optional[float]] = {'wfv': None, 'online': None}
        total_days = len(df)
        ...
        min_total_days = max(100, (horizon + 10) * 5)
        
        if total_days < min_total_days:
            logger.warning(f"âš ï¸ {symbol} {horizon}d: Insufficient data for evaluation ({total_days} days, need {min_total_days})")
            return results
```

HPO ile aynÄ± WFV split Ã¼retimi ve `evaluation_spec` eÅŸleÅŸtirmesi:

```1984:2009:scripts/continuous_hpo_training_pipeline.py
        # âš¡ NEW: Use multiple splits for walk-forward validation (same as HPO)
        from scripts.optuna_hpo_with_feature_flags import generate_walkforward_splits, calculate_dynamic_split
        wfv_splits = generate_walkforward_splits(total_days, horizon, n_splits=4)
        ...
        # âœ… NEW: If evaluation_spec present in HPO JSON, override splits and thresholds to ensure parity
        eval_spec = None
        ...
        if isinstance(eval_spec, dict):
            # Set DirHit threshold from spec (fallback to default)
            ...
            # Optionally mirror mask thresholds for any gating logic downstream
            try:
                if 'min_mask_count' in eval_spec:
                    os.environ['HPO_MIN_MASK_COUNT'] = str(int(eval_spec['min_mask_count']))
                if 'min_mask_pct' in eval_spec:
                    os.environ['HPO_MIN_MASK_PCT'] = str(float(eval_spec['min_mask_pct']))
            except Exception:
                pass
            # Override WFV splits using indices if provided
            ...
```

Ve WFV DirHit ortalamasÄ±:

```2487:2519:scripts/continuous_hpo_training_pipeline.py
            # Average DirHit, nRMSE, and Score across all splits
            # âœ… FIX: Require at least 2 splits for reliable DirHit calculation (same as HPO)
            # Single split DirHit is statistically unreliable
            if len(split_dirhits) >= 2:
                avg_dirhit = float(np.mean(split_dirhits))
                avg_nrmse = float(np.mean(split_nrmses)) if split_nrmses else float('inf')
                avg_score = float(np.mean(split_scores)) if split_scores else 0.0
                logger.info(f"âœ… {symbol} {horizon}d WFV: Average across {len(split_dirhits)} splits: DirHit={avg_dirhit:.2f}%, nRMSE={avg_nrmse:.3f}, Score={avg_score:.2f}")
                results['wfv'] = avg_dirhit
                results['wfv_nrmse'] = avg_nrmse
                results['wfv_score'] = avg_score
            elif len(split_dirhits) == 1:
                ...
                results['wfv'] = None
                ...
                results['low_support_warning'] = True
            else:
                ...
                results['wfv'] = None
                ...
                results['low_support_warning'] = True
                # âœ… FALLBACK: Try to find best params with 0/0.0 filter (no filter)
                if hpo_result and 'json_file' in hpo_result:
                    ...
                    fallback_params = find_fallback_best_params(study_db, symbol, horizon)
                    if fallback_params:
                        ...
                        results['fallback_best_params'] = fallback_params
                        results['fallback_available'] = True
```

**Ã–zet:**
- EÄŸitim sonrasÄ± DirHit, **HPO ile aynÄ± WFV mantÄ±ÄŸÄ±yla** (aynÄ± splitâ€™ler, aynÄ± filtreler) hesaplanÄ±yor.
- En az **2 split** zorunluluÄŸu var; aksi durumda DirHit **LOW_SUPPORT** olarak iÅŸaretleniyor.
- EÄŸer filtreye takÄ±lan semboller varsa, **0/0.0 fallback** ile studyâ€™den en iyi trial parametreleri ek olarak bulunup `results['fallback_best_params']` iÃ§inde raporlanÄ±yor.

---

### 7. DÃ¼ÅŸÃ¼k Destek Fallback MantÄ±ÄŸÄ±

#### 7.1. Study TabanlÄ± Fallback (find_fallback_best_params.py)

```23:93:scripts/find_fallback_best_params.py
def find_fallback_best_params(study_db: Path, symbol: str, horizon: int) -> Optional[Dict]:
    """Find best params using 0/0.0 filter (no filter) as fallback
    ...
    """
    try:
        study = optuna.load_study(
            study_name=None,
            storage=f"sqlite:///{study_db}"
        )
        
        symbol_key = f"{symbol}_{horizon}d"
        best_trial = None
        best_filtered_score = float('-inf')
        
        for trial in study.trials:
            if trial.state != optuna.trial.TrialState.COMPLETE:
                continue
            # Get split metrics
            symbol_metrics = trial.user_attrs.get('symbol_metrics', {})
            if symbol_key not in symbol_metrics:
                continue
            split_metrics = symbol_metrics[symbol_key].get('split_metrics', [])
            if not split_metrics:
                continue
            # Apply 0/0.0 filter (no filter) - include all splits
            filtered_dirhits = []
            for split in split_metrics:
                dirhit = split.get('dirhit')
                if dirhit is not None:
                    filtered_dirhits.append(dirhit)
            # Need at least 1 split
            if len(filtered_dirhits) == 0:
                continue
            # Calculate filtered average DirHit
            filtered_score = sum(filtered_dirhits) / len(filtered_dirhits)
            if filtered_score > best_filtered_score:
                best_filtered_score = filtered_score
                best_trial = trial
        ...
        return {
            'best_params': best_params,
            'best_trial_number': best_trial.number,
            'best_value': best_filtered_score,
            'features_enabled': features_enabled,
            'feature_params': feature_params,
            'filter_used': {'min_mask_count': 0, 'min_mask_pct': 0.0},  # Fallback filter
            'is_fallback': True
        }
```

Bu fallback:
- **HiÃ§bir split 5/2.5 filtresini geÃ§emeyen** semboller iÃ§in,
- En az 1 split DirHitâ€™i kullanarak 0/0.0 filtre ile **en iyi trialâ€™Ä±** bulur,
- Bu parametreler **LOW_SUPPORT uyarÄ±sÄ±yla** birlikte training deÄŸerlendirmesine eklenir.

#### 7.2. JSON TabanlÄ± Fallback (recreate_all_json_from_study_with_filter.py)

```280:331:scripts/recreate_all_json_from_study_with_filter.py
def create_json_from_filtered_trial(db_file: Path, symbol: str, horizon: int, cycle: int,
                                    min_mask_count: int = 5, min_mask_pct: float = 2.5,
                                    min_valid_splits: int = 2, dry_run: bool = False,
                                    timeout_seconds: int = 300) -> Optional[Path]:
    """Create JSON file from study database using filtered best trial"""
    ...
    filtered_trial, filtered_score = find_best_trial_with_timeout(
        db_file, symbol, horizon, min_mask_count, min_mask_pct, timeout_seconds
    )
    ...
    # Get symbol-specific avg_dirhit
    symbol_key = f"{symbol}_{horizon}d"
    symbol_metrics = filtered_trial.user_attrs.get('symbol_metrics', {})
    symbol_metric = symbol_metrics.get(symbol_key, {}) if isinstance(symbol_metrics, dict) else {}
    symbol_avg_dirhit = symbol_metric.get('avg_dirhit') if isinstance(symbol_metric, dict) else None
    ...
    if symbol_avg_dirhit is not None:
        best_dirhit = float(symbol_avg_dirhit)
    else:
        best_dirhit = filtered_score
    ...
```

Bu script:
- HPO studyâ€™den **5/2.5 ve minimum 2 split** ÅŸartlarÄ±nÄ± saÄŸlayan trialâ€™Ä± bularak JSON Ã¼retir.
- EÄŸer bÃ¶yle bir trial yoksa, Ã¼st seviyede **0/0.0 fallback** ile tekrar denenir (komut seviyesinde).

---

### 8. AkÄ±ÅŸ ve Zincir (UÃ§tan Uca Ã–zet)

1. **Veri katmanÄ±**:
   - Sembollerin OHLC verileri PostgreSQLâ€™de tutulur.
   - HPO ve training bu veriyi `fetch_prices` vb. fonksiyonlarla Ã§eker.
2. **HPO sÃ¼reci (optuna_hpo_with_feature_flags.py)**:
   - Her trial iÃ§in:
     - Feature flags + iÃ§ parametreler + hyperparametreler Ã¶rneklenir.
     - En az 100 gÃ¼n veri varsa 4 WFV split Ã¼retilir.
     - Her split iÃ§in model eÄŸitilir, DirHit ve nRMSE hesaplanÄ±r.
     - En az 2 split varsa DirHit ortalamasÄ± alÄ±nÄ±r; aksi durumda DirHit sembol seviyesinde **LOW_SUPPORT** olur.
     - TÃ¼m sembollerin DirHitâ€™leri `avg_dirhit` olarak, sembol bazlÄ± detaylar `symbol_metrics` olarak kaydedilir.
3. **HPO orkestrasyonu (ContinuousHPOPipeline.run_hpo)**:
   - HPO scriptâ€™i subprocess olarak baÅŸlatÄ±lÄ±r (CPU affinity, slot kontrolÃ¼, log dosyalarÄ±).
   - Ã‡Ä±ktÄ± JSONâ€™larÄ±:
     - **5/2.5 + min_valid_splits â‰¥ 2** filtrelerine gÃ¶re,
     - LOW_SUPPORT adaylar ikinci planda olmak Ã¼zere,
     - DirHit â†’ score â†’ zaman Ã¶nceliÄŸiyle seÃ§ilir.
4. **Durum gÃ¼ncelleme (TaskState)**:
   - HPO tamamlanÄ±nca `hpo_completed_at`, `hpo_dirhit`, `best_params_file`, `cycle` gÃ¼ncellenir.
5. **Training (run_training)**:
   - HPOâ€™dan gelen `best_params`, `features_enabled`, `feature_params` ortam deÄŸiÅŸkenlerine yazÄ±lÄ±r.
   - Adaptive learning **kapalÄ±**, feature bayraklarÄ± HPO ile uyumlu.
   - EÄŸitim yapÄ±lÄ±r, model diskâ€™e kaydedilir.
6. **EÄŸitim sonrasÄ± deÄŸerlendirme (_evaluate_training_dirhits)**:
   - HPO ile aynÄ± WFV splitâ€™leri (`evaluation_spec` ile tam eÅŸleÅŸtirilmiÅŸ) kullanÄ±lÄ±r.
   - En az 2 split varsa DirHit hesaplanÄ±r; deÄŸilse WFV DirHit `None` + LOW_SUPPORT flag.
   - Gerekirse `find_fallback_best_params` ile 0/0.0 fallback parametreler bulunur ve raporlanÄ±r.
7. **State & raporlama**:
   - `continuous_hpo_state.json` iÃ§inde:
     - `status`, `hpo_dirhit`, `training_dirhit_wfv`, `training_dirhit_online`, `adaptive_dirhit`,
     - `cycle`, `retry_count`, `error` gibi alanlar gÃ¼ncel tutulur.
   - `show_hpo_progress.py`:
     - Hem state hem de study dosyalarÄ±ndan,
     - **sembol spesifik** DirHitâ€™i `symbol_metrics[symbol_key]['avg_dirhit']` Ã¼zerinden okur,
     - HPO vs Training DirHit karÅŸÄ±laÅŸtÄ±rmasÄ±nÄ± ekrana basar.
8. **Cycle dÃ¶ngÃ¼sÃ¼**:
   - TÃ¼m semboller tÃ¼m horizonâ€™lar iÃ§in tamamlandÄ±ÄŸÄ±nda cycle artar.
   - Eski cycleâ€™a ait study/JSON dosyalarÄ± temizlenir.
   - Yeni cycleâ€™da **gÃ¼ncellenmiÅŸ veriyle** HPO yeniden Ã§alÄ±ÅŸÄ±r; bu da incremental Ã¶ÄŸrenme etkisi yaratÄ±r.

---

### 9. MantÄ±ksal ve Kural BazlÄ± DeÄŸerlendirme

#### 9.1. MantÄ±ksal TutarlÄ±lÄ±k

- **Sembol bazlÄ± HPO**:
  - Pipeline her HPO Ã§aÄŸrÄ±sÄ±nda **tek sembol** gÃ¶nderiyor (`--symbols {SYMBOL}`), bu yÃ¼zden:
    - `study.best_trial` o sembolâ€“ufuk iÃ§in en iyi trial,
    - `symbol_metrics[symbol_key]['avg_dirhit']` de **doÄŸrudan o sembole ait** DirHit.
  - `show_hpo_progress.py` ve `continuous_hpo_training_pipeline.py` artÄ±k DirHitâ€™i **bu sembol spesifik metrikten** okuyor; bu, mantÄ±ksal olarak doÄŸru.

- **Filtre ve split sayÄ±sÄ±**:
  - HPO ve trainingâ€™de **aynÄ± DirHit tanÄ±mÄ±** ve **aynÄ± WFV split stratejisi** kullanÄ±lÄ±yor.
  - Hem HPO, hem training iÃ§in **en az 2 split** ÅŸartÄ± var; tek split sonuÃ§larÄ± istatistiksel olarak gÃ¼vensiz olduÄŸu iÃ§in hariÃ§ tutuluyor.
  - Bu, kullanÄ±cÄ± talebiyle birebir uyumlu ve istatistiksel aÃ§Ä±dan mantÄ±klÄ±.

- **LOW_SUPPORT semboller**:
  - 5/2.5 filtresine takÄ±lan (mask_count/mask_pct dÃ¼ÅŸÃ¼k) semboller:
    - Ã–nce HPO JSON seÃ§iminde **deprioritize** ediliyor (varsa destekli adaylar tercih ediliyor).
    - HiÃ§ destekli aday yoksa:
      - HPO DirHit **uyarÄ± ile** kabul ediliyor,
      - Training DirHitâ€™in daha gÃ¼venilir olacaÄŸÄ± explicit log mesajlarÄ±yla belirtiliyor.
  - AyrÄ±ca `find_fallback_best_params` ile 0/0.0 filtreli fallback parametreler hesaplanÄ±yor; bu da **â€œhiÃ§ model kalmasÄ±nâ€** riskini azaltÄ±yor.

#### 9.2. Ä°ÅŸ/Kural Perspektifi

- **Veri kalitesi**:
  - Hem HPO hem training tarafÄ±nda minimum gÃ¼n sayÄ±sÄ± ve mask filtreleriyle veri kalitesi korunuyor.
  - **En az iki split** ile karar verilmesi, iÅŸ aÃ§Ä±sÄ±ndan **daha stabil ve gÃ¼venilir** bir performans Ã¶lÃ§Ã¼mÃ¼ saÄŸlÄ±yor.

- **SÃ¼reklilik ve geri kazanÄ±m (resilience)**:
  - HPO yarÄ±m kaldÄ±ÄŸÄ±nda:
    - Study dosyasÄ±ndan trial sayÄ±sÄ± kontrol ediliyor.
    - Yeterli trial yoksa, aynÄ± studyâ€™den **warmâ€‘start** ile devam ediliyor.
    - Yeterli trial varsa, JSON olmasa bile studyâ€™den JSON **recovery** yapÄ±lÄ±yor.
  - Bu, kurumsal ortamda beklenen **dayanÄ±klÄ±lÄ±k ve otomatik toparlanma** davranÄ±ÅŸÄ±na uygun.

- **Cycle yÃ¶netimi**:
  - Cycle numarasÄ± sadece **mevcut cycle tamamen bittiÄŸinde** artÄ±yor; bu, raporlama ve versiyonlama iÃ§in mantÄ±klÄ±.
  - Eski cycle dosyalarÄ±nÄ±n otomatik temizlenmesi, disk kullanÄ±mÄ±nÄ± ve kafa karÄ±ÅŸÄ±klÄ±ÄŸÄ±nÄ± azaltÄ±yor.

- **Ã–lÃ§eklenebilirlik ve kaynak yÃ¶netimi**:
  - Global HPO slotâ€™larÄ±, NUMA/CPU affinity, log yÃ¶nlendirme gibi detaylar:
    - YÃ¼ksek sembol sayÄ±sÄ±nda bile sistemi **stabil ve performanslÄ±** tutmayÄ± hedefliyor.

- **ÅeffaflÄ±k ve denetlenebilirlik**:
  - TÃ¼m kritik kararlar:
    - HPO JSONâ€™larÄ±,
    - Study dosyalarÄ±,
    - `continuous_hpo_state.json`,
    - `show_hpo_progress.py` Ã§Ä±ktÄ±larÄ± Ã¼zerinden izlenebilir.
  - LOW_SUPPORT durumu, fallback kullanÄ±mÄ± ve filtre deÄŸerleri logâ€™larda ve stateâ€™te aÃ§Ä±kÃ§a iÅŸaretleniyor.

#### 9.3. Potansiyel GeliÅŸtirme AlanlarÄ±

- LOW_SUPPORT durumunda:
  - Åu an fallback 0/0.0 DirHit ile Ã§alÄ±ÅŸÄ±yor; bu, veri Ã§ok zayÄ±fsa halen riskli olabilir.  
    - GeliÅŸtirme: **min split sayÄ±sÄ± â‰¥ 2** kuralÄ±nÄ± fallbackâ€™te de zorlamak veya DirHit yerine **nRMSE aÄŸÄ±rlÄ±klÄ±** bir skor kullanmak.
- Ä°ÅŸ kuralÄ± olarak:
  - Belirli bir eÅŸik altÄ±ndaki DirHitâ€™ler iÃ§in (Ã¶rneÄŸin < 45%) modelin **otomatik olarak â€œkullanÄ±lamazâ€ iÅŸaretlenmesi** (Ã¶rneÄŸin prediction sisteminde devre dÄ±ÅŸÄ± bÄ±rakma) dÃ¼ÅŸÃ¼nÃ¼lebilir.

Genel olarak, mevcut mimari:
- **Sembol bazlÄ±**,  
- **WFV temelli**,  
- **filtre ve split sayÄ±sÄ± aÃ§Ä±sÄ±ndan tutarlÄ±**,  
- **cycle bazlÄ± incremental iyileÅŸme** saÄŸlayan,  
mantÄ±ksal ve kurumsal aÃ§Ä±dan gÃ¼Ã§lÃ¼ bir HPO + Training zinciri sunuyor.


