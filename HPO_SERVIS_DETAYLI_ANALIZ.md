# HPO Servisi DetaylÄ± Analiz

## ğŸ“‹ Genel BakÄ±ÅŸ

HPO (Hyperparameter Optimization) servisi, BIST hisse senetleri iÃ§in makine Ã¶ÄŸrenmesi modellerinin optimizasyonunu otomatikleÅŸtiren kapsamlÄ± bir sistemdir. Sistem, **feature flags**, **feature internal parameters** ve **model hyperparameters**'Ä± birlikte optimize ederek en iyi performansÄ± hedefler.

---

## ğŸ—ï¸ Mimari YapÄ±

### 1. Ana BileÅŸenler

#### 1.1. `optuna_hpo_with_feature_flags.py` - HPO Objective Function
**Rol:** Optuna ile hyperparameter optimization yapan ana script

**Temel Ä°ÅŸlevler:**
- **Feature Flag Optimization:** 11 adet feature flag'i optimize eder
  - `ENABLE_EXTERNAL_FEATURES`, `ENABLE_FINGPT_FEATURES`, `ENABLE_YOLO_FEATURES`
  - `ML_USE_DIRECTIONAL_LOSS`, `ENABLE_SEED_BAGGING`, `ENABLE_TALIB_PATTERNS`
  - `ML_USE_SMART_ENSEMBLE`, `ML_USE_STACKED_SHORT`, `ENABLE_META_STACKING`
  - `ML_USE_REGIME_DETECTION`, `ENABLE_FINGPT`
  
- **Feature Internal Parameters:** Feature'lar aÃ§Ä±kken optimize edilen iÃ§ parametreler
  - Directional Loss: `ml_loss_mse_weight`, `ml_loss_threshold`, `ml_dir_penalty`
  - Seed Bagging: `n_seeds`
  - Meta Stacking: `meta_stacking_alpha`
  - Adaptive Learning: `ml_adaptive_k_{horizon}d`, `ml_pattern_weight_scale_{horizon}d`
  - YOLO: `yolo_min_conf`
  - Smart Ensemble: `smart_consensus_weight`, `smart_performance_weight`, `smart_sigma`, `smart_weight_xgb/lgbm/cat`
  - FinGPT: `fingpt_confidence_threshold`
  - External Features: `external_min_days`, `external_smooth_alpha`
  - Regime Detection: `regime_scale_low`, `regime_scale_high`

- **Model Hyperparameters:** XGBoost, LightGBM, CatBoost parametreleri
  - XGBoost: `n_estimators`, `max_depth`, `learning_rate`, `subsample`, `colsample_bytree`, `reg_alpha`, `reg_lambda`, `min_child_weight`, `gamma`, `grow_policy`, `tree_method`, `max_bin`
  - LightGBM: `n_estimators`, `max_depth`, `learning_rate`, `num_leaves`, `subsample`, `colsample_bytree`, `reg_alpha`, `reg_lambda`, `min_data_in_leaf`, `feature_fraction_bynode`, `bagging_freq`, `min_gain_to_split`
  - CatBoost: `iterations`, `depth`, `learning_rate`, `l2_leaf_reg`, `subsample`, `rsm`, `border_count`, `random_strength`, `leaf_estimation_iterations`, `bootstrap_type`

**Optimizasyon Metrikleri:**
- **Primary Metric:** DirHit (Directional Hit Rate) - YÃ¶n tahmin doÄŸruluÄŸu
- **Secondary Metric:** nRMSE (normalized RMSE) - Normalize edilmiÅŸ hata
- **Combined Score:** `0.7 * DirHit - k * nRMSE` (k=6.0 for short horizons, k=4.0 for long)

**Walk-Forward Validation:**
- 4 split walk-forward validation kullanÄ±r
- Her split iÃ§in train/test ayrÄ±mÄ± yapÄ±lÄ±r
- TÃ¼m split'lerin DirHit ortalamasÄ± alÄ±nÄ±r

**Data Leakage Ã–nleme:**
- `ML_USE_ADAPTIVE_LEARNING = '0'` (HPO sÄ±rasÄ±nda her zaman kapalÄ±)
- `ML_SKIP_ADAPTIVE_PHASE2 = '1'` (Phase 2 skip)
- Model her split iÃ§in sÄ±fÄ±rdan eÄŸitilir (test verisi gÃ¶rÃ¼lmez)

**Study Management:**
- SQLite database'de study dosyasÄ± saklanÄ±r
- Cycle number ile study isimlendirilir: `hpo_with_features_{symbol}_h{horizon}_c{cycle}`
- WAL mode aktif (concurrent read/write iÃ§in)
- Warm-start: Ã–nceki cycle'larÄ±n best params'larÄ± enqueue edilir

#### 1.2. `continuous_hpo_training_pipeline.py` - Pipeline Orchestrator
**Rol:** HPO ve training sÃ¼reÃ§lerini koordine eden ana orchestrator

**Temel Ä°ÅŸlevler:**

**Cycle Management:**
- Her cycle: TÃ¼m semboller iÃ§in tÃ¼m horizonlar (1d, 3d, 7d, 14d, 30d)
- Cycle tamamlandÄ±ktan sonra yeni verilerle tekrar baÅŸlar (incremental learning)
- State file ile progress tracking (`continuous_hpo_state.json`)

**Processing Strategy:**
- **Horizon-First Approach:** TÃ¼m semboller iÃ§in bir horizon bitirilir, sonra diÄŸer horizon'a geÃ§ilir
  - Phase 1: TÃ¼m semboller iÃ§in 1d
  - Phase 2: TÃ¼m semboller iÃ§in 3d
  - Phase 3: TÃ¼m semboller iÃ§in 7d
  - Phase 4: TÃ¼m semboller iÃ§in 14d
  - Phase 5: TÃ¼m semboller iÃ§in 30d
- **Parallelism:** `MAX_WORKERS` (default: 4) sembol paralel iÅŸlenir
- **Sequential per Symbol:** Her sembol iÃ§in horizonlar sÄ±rayla iÅŸlenir (1dâ†’3dâ†’7dâ†’14dâ†’30d)

**Task States:**
- `pending`: HenÃ¼z baÅŸlamamÄ±ÅŸ
- `hpo_in_progress`: HPO Ã§alÄ±ÅŸÄ±yor
- `training_in_progress`: Training Ã§alÄ±ÅŸÄ±yor
- `completed`: BaÅŸarÄ±yla tamamlandÄ±
- `failed`: BaÅŸarÄ±sÄ±z (retry mekanizmasÄ± var)
- `skipped`: Yetersiz veri nedeniyle atlandÄ±

**HPO Execution:**
- Subprocess olarak `optuna_hpo_with_feature_flags.py` Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r
- HPO slot locking (fcntl) ile concurrency kontrolÃ¼
- CPU affinity optimization (NUMA-aware)
- Timeout: 72 saat (1500 trials iÃ§in)
- JSON file validation ve recovery mekanizmasÄ±

**Training Execution:**
- Best params ile full training
- Adaptive learning KAPALI (HPO ile tutarlÄ±lÄ±k)
- Model kaydetme ve doÄŸrulama
- DirHit evaluation (WFV ve online)

**DirHit Evaluation:**
- **WFV (Walk-Forward Validation):** Adaptive OFF, best params ile yeniden eÄŸitim
- **Online:** Adaptive OFF, best params ile prediction
- HPO DirHit ile karÅŸÄ±laÅŸtÄ±rma (alignment check)

**Recovery Mechanisms:**
- HPO completed ama JSON missing: Study file'dan recovery
- State file corruption: Rebuild from study files
- Stale in-progress tasks: Reset to pending after restart

#### 1.3. `train_completed_hpo_with_best_params.py` - Training Script
**Rol:** HPO tamamlanmÄ±ÅŸ semboller iÃ§in best params ile training

**Temel Ä°ÅŸlevler:**
- Completed HPO JSON dosyalarÄ±nÄ± okur
- Best params'Ä± environment variables'a set eder
- Full training yapar (tÃ¼m feature'lar aÃ§Ä±k)
- Walk-forward validation ile DirHit hesaplar
- Model kaydetme

**CatBoost Bootstrap Type Normalization:**
- `_normalize_cat_bootstrap_type()` helper function
- Optuna'dan gelen bootstrap_type deÄŸerlerini CatBoost enum'larÄ±na normalize eder
- Mapping: `'Bayesian'`, `'Bernoulli'`, `'MVS'`, `'No'`

#### 1.4. `enhanced_ml_system.py` - ML System
**Rol:** HPO params'larÄ± kullanan ML training ve prediction sistemi

**HPO Param Integration:**
- Environment variables'dan `OPTUNA_XGB_*`, `OPTUNA_LGB_*`, `OPTUNA_CAT_*` okur
- `ConfigManager.get()` ile parametreleri alÄ±r
- Default deÄŸerler override edilir

**Feature Flag Integration:**
- `ENABLE_*` flags ile feature'lar aÃ§Ä±k/kapalÄ±
- `ML_USE_*` flags ile ML Ã¶zellikleri kontrol edilir
- Feature internal parameters environment variables'dan okunur

---

## ğŸ”„ Ä°ÅŸ AkÄ±ÅŸÄ± (Workflow)

### 2.1. HPO Workflow

```
1. Pipeline baÅŸlatÄ±lÄ±r
   â†“
2. Active symbols listesi alÄ±nÄ±r (database'den)
   â†“
3. Her symbol-horizon Ã§ifti iÃ§in:
   a. Data quality check (minimum 100 days)
   b. HPO slot acquire (concurrency control)
   c. Subprocess: optuna_hpo_with_feature_flags.py
      - Study file oluÅŸtur/load (cycle-aware)
      - Warm-start: Ã–nceki best params enqueue
      - 1500 trial optimization
        * Feature flags suggest
        * Feature params suggest (conditional)
        * Hyperparameters suggest
        * Environment variables set
        * EnhancedMLSystem instance create
        * Walk-forward validation (4 splits)
        * DirHit + nRMSE calculate
        * Score = 0.7 * DirHit - k * nRMSE
      - Best trial seÃ§ilir
      - JSON file save
   d. HPO slot release
   â†“
4. Best params ile training
   a. Best params environment'a set
   b. Feature flags set
   c. Adaptive learning OFF (HPO ile tutarlÄ±lÄ±k)
   d. EnhancedMLSystem.train_enhanced_models()
   e. Model save
   f. DirHit evaluation (WFV + online)
   â†“
5. State update (completed)
```

### 2.2. Cycle Management

```
Cycle 1:
  - TÃ¼m semboller iÃ§in tÃ¼m horizonlar HPO + Training
  - State file'da cycle=1 olarak kaydedilir
  
Cycle 2 (Yeni veriler eklendikten sonra):
  - Cycle number increment (cycle=2)
  - TÃ¼m semboller iÃ§in yeni HPO (yeni verilerle)
  - Study file: hpo_with_features_{symbol}_h{horizon}_c2.db
  - Ã–nceki cycle'Ä±n best params'larÄ± warm-start olarak kullanÄ±lÄ±r
  - Training yeni best params ile yapÄ±lÄ±r
  
Cycle N:
  - SÃ¼rekli incremental learning
  - Her cycle'da yeni verilerle HPO
  - Model performance iyileÅŸmesi
```

---

## ğŸ¯ MantÄ±k ve TasarÄ±m KararlarÄ±

### 3.1. Neden Feature Flags + Hyperparameters Birlikte?

**Problem:** Feature'larÄ±n aÃ§Ä±k/kapalÄ± durumu model performansÄ±nÄ± etkiler. Ã–rneÄŸin:
- Directional Loss aÃ§Ä±kken: `ml_loss_mse_weight`, `ml_loss_threshold` optimize edilmeli
- Seed Bagging aÃ§Ä±kken: `n_seeds` optimize edilmeli
- Smart Ensemble aÃ§Ä±kken: `smart_consensus_weight`, `smart_performance_weight` optimize edilmeli

**Ã‡Ã¶zÃ¼m:** Feature flags ve hyperparameters birlikte optimize edilir. Bu sayede:
- Feature'Ä±n etkisi doÄŸru Ã¶lÃ§Ã¼lÃ¼r
- Feature aÃ§Ä±kken optimize edilen parametreler kullanÄ±lÄ±r
- Feature kapalÄ±yken gereksiz parametreler optimize edilmez

### 3.2. Neden Adaptive Learning HPO'da KapalÄ±?

**Problem:** Adaptive learning, model'in test verisini gÃ¶rmesine izin verir. Bu data leakage'dÄ±r.

**Ã‡Ã¶zÃ¼m:** HPO sÄ±rasÄ±nda adaptive learning her zaman kapalÄ±:
- `ML_USE_ADAPTIVE_LEARNING = '0'`
- Model sadece train verisi ile eÄŸitilir
- Test verisi sadece evaluation iÃ§in kullanÄ±lÄ±r
- HPO DirHit gerÃ§ekÃ§i bir metrik olur

**Training'de de KapalÄ± (Hibrit YaklaÅŸÄ±m):**
- Plan'a gÃ¶re: HPO ve Training aynÄ± veri miktarÄ±nÄ± kullanmalÄ±
- Cycle zaten incremental learning etkisi yaratÄ±yor (yeni verilerle yeniden HPO)
- Adaptive learning yerine cycle-based incremental learning kullanÄ±lÄ±yor

### 3.3. Neden Walk-Forward Validation?

**Problem:** Single split validation overfitting riski taÅŸÄ±r.

**Ã‡Ã¶zÃ¼m:** 4 split walk-forward validation:
- Her split iÃ§in train/test ayrÄ±mÄ±
- Expanding window approach
- Ortalama DirHit daha gÃ¼venilir
- Overfitting riski azalÄ±r

### 3.4. Neden Cycle-Based Study Files?

**Problem:** Yeni veriler eklendiÄŸinde, eski study file'a yazmak karÄ±ÅŸÄ±klÄ±ÄŸa neden olur.

**Ã‡Ã¶zÃ¼m:** Cycle number ile study file isimlendirme:
- `hpo_with_features_{symbol}_h{horizon}_c{cycle}.db`
- Her cycle kendi study file'Ä±na sahip
- Ã–nceki cycle'lar korunur (analiz iÃ§in)
- Warm-start: Ã–nceki cycle'Ä±n best params'larÄ± kullanÄ±lÄ±r

### 3.5. Neden Horizon-First Processing?

**Problem:** Symbol-first processing'de:
- 1d iÃ§in tÃ¼m semboller bitene kadar 3d baÅŸlamaz
- KullanÄ±cÄ± 1d sonuÃ§larÄ±nÄ± bekler

**Ã‡Ã¶zÃ¼m:** Horizon-first processing:
- TÃ¼m semboller iÃ§in 1d bitirilir â†’ 1d sonuÃ§larÄ± hazÄ±r
- Sonra tÃ¼m semboller iÃ§in 3d â†’ 3d sonuÃ§larÄ± hazÄ±r
- Incremental value delivery: KÄ±sa horizonlar Ã¶nce hazÄ±r olur

### 3.6. Neden Symbol-Based Sequential (Her Symbol Ä°Ã§in)?

**Problem:** Her horizon iÃ§in tÃ¼m semboller paralel iÅŸlenirse:
- Database yÃ¼kÃ¼ artar (aynÄ± sembol iÃ§in veri birden fazla kez Ã§ekilir)
- SQLite Ã§akÄ±ÅŸmalarÄ± olur (aynÄ± study file'a yazma)

**Ã‡Ã¶zÃ¼m:** Symbol-based sequential:
- Her sembol iÃ§in tÃ¼m horizonlar sÄ±rayla iÅŸlenir (1dâ†’3dâ†’7dâ†’14dâ†’30d)
- AynÄ± sembol iÃ§in veri bir kez Ã§ekilir
- SQLite Ã§akÄ±ÅŸmalarÄ± azalÄ±r (bir sembol at a time)
- MAX_WORKERS: Semboller paralel, her biri sequential

### 3.7. Neden DirHit + nRMSE Combined Score?

**Problem:** Sadece DirHit optimize edilirse:
- Model yÃ¼ksek confidence ile yanlÄ±ÅŸ tahmin yapabilir
- RMSE yÃ¼ksek olabilir (bÃ¼yÃ¼k hatalar)

**Ã‡Ã¶zÃ¼m:** Combined score:
- `score = 0.7 * DirHit - k * nRMSE`
- DirHit: YÃ¶n doÄŸruluÄŸu (primary)
- nRMSE: Normalize edilmiÅŸ hata (secondary)
- k=6.0 (short horizons), k=4.0 (long horizons)
- Hem yÃ¶n doÄŸruluÄŸu hem de hata miktarÄ± optimize edilir

### 3.8. Neden Seed Matching (HPO vs Training)?

**Problem:** HPO'da farklÄ± seed, training'de farklÄ± seed kullanÄ±lÄ±rsa:
- DirHit farklÄ±lÄ±klarÄ± seed'den kaynaklanabilir
- HPO DirHit ile Training DirHit karÅŸÄ±laÅŸtÄ±rÄ±lamaz

**Ã‡Ã¶zÃ¼m:** Best trial'Ä±n seed'i kullanÄ±lÄ±r:
- HPO: `ml.base_seeds = [42 + trial.number]`
- Training: `ml.base_seeds = [42 + best_trial_number]`
- Evaluation: `ml_eval.base_seeds = [42 + best_trial_number]`
- Seed matching ile DirHit karÅŸÄ±laÅŸtÄ±rmasÄ± gÃ¼venilir olur

### 3.9. Neden CatBoost Bootstrap Type Normalization?

**Problem:** Optuna `suggest_categorical()` string dÃ¶ner, CatBoost enum bekler.

**Ã‡Ã¶zÃ¼m:** Normalization helper:
- `'Bayesian'` â†’ `'Bayesian'`
- `'Bernoulli'` â†’ `'Bernoulli'`
- `'MVS'` â†’ `'MVS'`
- `'No'` â†’ `'No'`
- Invalid deÄŸerler skip edilir (model default kullanÄ±r)

---

## ğŸ”§ Teknik Detaylar

### 4.1. Concurrency Control

**HPO Slot Locking:**
- `acquire_hpo_slot()`: fcntl ile file-based locking
- `HPO_MAX_SLOTS` (default: 3) slot mevcut
- Her HPO process bir slot acquire eder
- Slot dolduÄŸunda blocking wait

**State File Locking:**
- Read: Shared lock (`LOCK_SH`)
- Write: Exclusive lock (`LOCK_EX`)
- Atomic write: Temp file + `os.replace()`
- Merge-aware: Concurrent processes'in state'lerini merge eder

### 4.2. CPU Affinity Optimization

**NUMA-Aware:**
- 4 NUMA node, her biri 32 CPU
- Round-robin NUMA node assignment
- `taskset` ile CPU affinity binding
- Process priority: `nice(-5)` (higher priority)

### 4.3. Memory Management

**Memory Leak Prevention:**
- Her trial sonrasÄ±: `ml.models.clear()`
- Her 5 trial'da bir: `gc.collect()`
- Feature cache clearing
- Horizon features clearing

### 4.4. Error Handling

**Retry Mechanism:**
- HPO failed: 3 retry hakkÄ±
- Permanent failures: `skipped` (retry yok)
  - Insufficient data
  - Symbol not found
  - Delisted
- Temporary failures: Retry
  - Timeout
  - Network errors
  - Subprocess errors

**Recovery Mechanisms:**
- HPO completed ama JSON missing: Study file'dan recovery
- State file corruption: Rebuild from study files
- Stale in-progress: Reset to pending

### 4.5. Data Quality Gates

**Minimum Data Requirements:**
- TÃ¼m horizonlar iÃ§in: 100 days minimum
- Test set iÃ§in: `horizon + 10` days minimum
- Walk-forward splits iÃ§in: Yeterli test data

**Data Validation:**
- Duplicate date kontrolÃ¼
- NaN/INF temizleme
- Cache bypass (HPO iÃ§in fresh data)

---

## ğŸ“Š Metrikler ve DeÄŸerlendirme

### 5.1. HPO Metrics

**Primary:**
- DirHit: YÃ¶n tahmin doÄŸruluÄŸu (%)
- nRMSE: Normalize edilmiÅŸ hata
- Score: `0.7 * DirHit - k * nRMSE`

**Secondary:**
- RMSE: Root mean squared error
- MAPE: Mean absolute percentage error
- Valid predictions count
- Threshold mask statistics

### 5.2. Training Metrics

**WFV DirHit:**
- Adaptive OFF
- Best params ile yeniden eÄŸitim
- Walk-forward validation
- HPO DirHit ile karÅŸÄ±laÅŸtÄ±rma

**Online DirHit:**
- Adaptive OFF
- Best params ile prediction
- Full dataset Ã¼zerinde

**Alignment Check:**
- WFV DirHit vs HPO DirHit
- Fark < 1%: âœ… Aligned
- Fark >= 1%: âš ï¸ Warning

---

## ğŸ¯ Hedefler ve Beklentiler

### 6.1. Optimizasyon Hedefleri

**Feature Flag Coverage:**
- 1500 trials â†’ ~73% feature flag combination coverage (1500/2048)
- 11 feature flag â†’ 2^11 = 2048 kombinasyon
- TPE sampler ile intelligent exploration

**Hyperparameter Space:**
- ~36-43 parametre optimize edilir
- 11 feature flag + 10-12 feature param + 15-20 hyperparam
- Conditional optimization (feature aÃ§Ä±kken optimize et)

### 6.2. Performance Hedefleri

**DirHit Improvement:**
- Her cycle'da DirHit artÄ±ÅŸÄ± beklenir
- Yeni verilerle incremental learning
- Best params ile training DirHit > HPO DirHit (adaptive learning etkisi)

**Training Time:**
- HPO: 72 saat (1500 trials)
- Training: ~5-10 dakika (best params ile)
- Total per symbol-horizon: ~72 saat

### 6.3. Scalability

**Parallel Processing:**
- MAX_WORKERS=4: 4 sembol paralel
- Her sembol sequential (horizonlar sÄ±rayla)
- HPO slot limiting: 3 concurrent HPO

**Resource Usage:**
- CPU: NUMA-aware binding
- Memory: Leak prevention
- Disk: Study files (SQLite), JSON results
- Database: PgBouncer connection pooling

---

## ğŸ” Kritik Noktalar ve Dikkat Edilmesi Gerekenler

### 7.1. Data Leakage Prevention

**HPO:**
- âœ… Adaptive learning OFF
- âœ… Phase 2 skip
- âœ… Walk-forward validation
- âœ… Test verisi sadece evaluation iÃ§in

**Training:**
- âœ… Adaptive learning OFF (HPO ile tutarlÄ±lÄ±k)
- âœ… Best params kullanÄ±mÄ±
- âœ… Seed matching

### 7.2. State Management

**State File:**
- Merge-aware writes (concurrent processes)
- Atomic writes (temp file + replace)
- Cycle preservation
- Recovery mechanisms

**Study Files:**
- Cycle-aware naming
- WAL mode (concurrent access)
- Recovery from study files

### 7.3. Parameter Alignment

**HPO â†’ Training:**
- Best params environment'a set
- Feature flags alignment
- Feature params alignment
- Seed matching

**Training â†’ Evaluation:**
- Best params kullanÄ±mÄ±
- Feature flags alignment
- Seed matching
- Adaptive learning OFF

### 7.4. Error Recovery

**HPO Failures:**
- Retry mechanism (3 attempts)
- Permanent vs temporary failures
- Study file recovery
- JSON file recovery

**Training Failures:**
- Model save verification
- Insufficient data handling
- Error logging and tracking

---

## ğŸ“ˆ SÃ¼reÃ§ AkÄ±ÅŸÄ± Ã–zeti

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HPO SERVÄ°SÄ° AKIÅI                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Pipeline BaÅŸlatma
   â”œâ”€ Active symbols listesi (database)
   â”œâ”€ State file load
   â””â”€ Cycle number belirleme

2. Horizon-First Processing
   â”œâ”€ Phase 1: TÃ¼m semboller iÃ§in 1d
   â”œâ”€ Phase 2: TÃ¼m semboller iÃ§in 3d
   â”œâ”€ Phase 3: TÃ¼m semboller iÃ§in 7d
   â”œâ”€ Phase 4: TÃ¼m semboller iÃ§in 14d
   â””â”€ Phase 5: TÃ¼m semboller iÃ§in 30d

3. Her Symbol-Horizon Ã‡ifti Ä°Ã§in:
   â”œâ”€ Data Quality Check (min 100 days)
   â”œâ”€ HPO Slot Acquire
   â”œâ”€ HPO Execution
   â”‚  â”œâ”€ Study file create/load (cycle-aware)
   â”‚  â”œâ”€ Warm-start (Ã¶nceki best params)
   â”‚  â”œâ”€ 1500 Trial Optimization
   â”‚  â”‚  â”œâ”€ Feature flags suggest
   â”‚  â”‚  â”œâ”€ Feature params suggest (conditional)
   â”‚  â”‚  â”œâ”€ Hyperparameters suggest
   â”‚  â”‚  â”œâ”€ Environment variables set
   â”‚  â”‚  â”œâ”€ EnhancedMLSystem create
   â”‚  â”‚  â”œâ”€ Walk-forward validation (4 splits)
   â”‚  â”‚  â”œâ”€ DirHit + nRMSE calculate
   â”‚  â”‚  â””â”€ Score = 0.7 * DirHit - k * nRMSE
   â”‚  â”œâ”€ Best trial select
   â”‚  â””â”€ JSON file save
   â”œâ”€ HPO Slot Release
   â”œâ”€ Training Execution
   â”‚  â”œâ”€ Best params set (env vars)
   â”‚  â”œâ”€ Feature flags set
   â”‚  â”œâ”€ Adaptive learning OFF
   â”‚  â”œâ”€ EnhancedMLSystem.train_enhanced_models()
   â”‚  â”œâ”€ Model save
   â”‚  â””â”€ DirHit evaluation (WFV + online)
   â””â”€ State Update (completed)

4. Cycle Completion
   â”œâ”€ TÃ¼m semboller iÃ§in tÃ¼m horizonlar tamamlandÄ±
   â”œâ”€ Cycle number increment
   â””â”€ Yeni verilerle tekrar baÅŸla (incremental learning)
```

---

## ğŸ“ Ã–ÄŸrenilen Dersler ve Best Practices

### 8.1. Data Leakage Prevention
- Adaptive learning HPO'da her zaman kapalÄ±
- Walk-forward validation kullan
- Test verisi sadece evaluation iÃ§in

### 8.2. Parameter Alignment
- HPO ve Training aynÄ± seed kullanmalÄ±
- Feature flags alignment kritik
- Best params environment'a doÄŸru set edilmeli

### 8.3. State Management
- Merge-aware writes (concurrent processes)
- Atomic writes (temp file + replace)
- Recovery mechanisms (study file, JSON file)

### 8.4. Error Handling
- Retry mechanism (temporary failures)
- Permanent failures â†’ skipped
- Recovery from study files

### 8.5. Performance Optimization
- CPU affinity (NUMA-aware)
- Memory leak prevention
- Concurrency control (HPO slots)

---

## ğŸ”® Gelecek Ä°yileÅŸtirmeler

### 9.1. Potansiyel Ä°yileÅŸtirmeler

**HPO Efficiency:**
- Early stopping (pruning) iyileÅŸtirmesi
- Parallel trials (Optuna distributed)
- Bayesian optimization tuning

**Training Efficiency:**
- Model caching
- Incremental training (sadece yeni verilerle)
- Distributed training

**Monitoring:**
- Real-time progress tracking
- DirHit trend analysis
- Resource usage monitoring

**Recovery:**
- Automatic recovery from failures
- State file backup
- Study file backup

---

## ğŸ“ SonuÃ§

HPO servisi, BIST hisse senetleri iÃ§in makine Ã¶ÄŸrenmesi modellerinin optimizasyonunu otomatikleÅŸtiren kapsamlÄ± bir sistemdir. Sistem:

1. **Feature flags, feature internal parameters ve hyperparameters**'Ä± birlikte optimize eder
2. **Walk-forward validation** ile gÃ¼venilir metrikler Ã¼retir
3. **Data leakage** Ã¶nleme mekanizmalarÄ± ile gerÃ§ekÃ§i deÄŸerlendirme yapar
4. **Cycle-based incremental learning** ile sÃ¼rekli iyileÅŸme saÄŸlar
5. **Horizon-first processing** ile incremental value delivery yapar
6. **Robust error handling** ve recovery mekanizmalarÄ± ile gÃ¼venilir Ã§alÄ±ÅŸÄ±r

Sistem, production-ready, scalable ve maintainable bir yapÄ±ya sahiptir.

