# HPO vs Training DirHit FarklarÄ± - DetaylÄ± Analiz Raporu

## ğŸ“Š Ã–zet

HPO sonuÃ§larÄ± ile Training sonuÃ§larÄ± arasÄ±nda Ã¶nemli farklar gÃ¶rÃ¼lÃ¼yor. Bu rapor, bu farklarÄ±n kÃ¶k nedenlerini analiz ediyor.

## ğŸ” Tespit Edilen Kritik Farklar

### 1. **Veri KaynaÄŸÄ± FarkÄ±** âœ… Ã‡Ã–ZÃœLMÃœÅ
- **HPO**: `fetch_prices()` kullanÄ±yor (DB'den direkt, cache bypass)
- **Training**: `fetch_prices()` kullanÄ±yor (aynÄ± kaynak) âœ…
- **Durum**: Kod incelemesinde training'in de `fetch_prices()` kullandÄ±ÄŸÄ± gÃ¶rÃ¼ldÃ¼ (satÄ±r 3197)

### 2. **Adaptive Learning Durumu** âœ… Ã‡Ã–ZÃœLMÃœÅ
- **HPO**: `ML_USE_ADAPTIVE_LEARNING = '0'` (kapalÄ±)
- **Training**: `ML_USE_ADAPTIVE_LEARNING = '0'` (kapalÄ±) âœ…
- **Durum**: Training'de de adaptive learning kapalÄ± (satÄ±r 3138)

### 3. **Seed KullanÄ±mÄ±** âš ï¸ POTANSÄ°YEL SORUN
- **HPO**: `ml.base_seeds = [42 + trial.number]` (her trial iÃ§in farklÄ± seed)
- **Training Evaluation**: `ml_eval.base_seeds = [42 + best_trial_number]` (best trial'Ä±n seed'i)
- **Durum**: Seed doÄŸru ayarlanÄ±yor gibi gÃ¶rÃ¼nÃ¼yor, ancak **model instance'larÄ± farklÄ±** olabilir

### 4. **Model Instance YÃ¶netimi** âš ï¸ KRÄ°TÄ°K SORUN
- **HPO**: Her trial iÃ§in **YENÄ°** `EnhancedMLSystem()` instance'Ä± oluÅŸturuluyor
- **Training Evaluation**: **YENÄ°** `EnhancedMLSystem()` instance'Ä± oluÅŸturuluyor âœ…
- **Ancak**: Training'de Ã¶nce tÃ¼m df ile model eÄŸitiliyor, sonra evaluation iÃ§in train_df ile yeniden eÄŸitiliyor
- **Sorun**: Ä°lk eÄŸitim (tÃ¼m df ile) evaluation'Ä± etkileyebilir mi?

### 5. **Split Stratejisi** âœ… AYNI
- **HPO**: `generate_walkforward_splits(total_days, horizon, n_splits=4)` - 4 split
- **Training Evaluation**: `generate_walkforward_splits(total_days, horizon, n_splits=4)` - 4 split âœ…
- **Durum**: AynÄ± split fonksiyonu kullanÄ±lÄ±yor

### 6. **Evaluation Spec KullanÄ±mÄ±** âš ï¸ KONTROL EDÄ°LMELÄ°
- **Training**: `evaluation_spec` varsa split'leri override ediyor
- **Sorun**: EÄŸer `evaluation_spec` yoksa veya farklÄ±ysa, split'ler farklÄ± olabilir

### 7. **Low Support Gating** âœ… DÃœZELTÄ°LDÄ°
- **HPO**: `HPO_MIN_MASK_COUNT` ve `HPO_MIN_MASK_PCT` kontrolÃ¼ yapÄ±lÄ±yor (default: 0, 0)
- **Training**: `HPO_MIN_MASK_COUNT` ve `HPO_MIN_MASK_PCT` kontrolÃ¼ yapÄ±lÄ±yor (default: 0, 0) âœ…
- **Durum**: ArtÄ±k aynÄ± default deÄŸerler kullanÄ±lÄ±yor (dÃ¼zeltme yapÄ±ldÄ±)

### 8. **DirHit Hesaplama MantÄ±ÄŸÄ±** âœ… AYNI
- **HPO**: `dirhit(y_true, y_pred, thr=0.005)` - threshold mask kullanÄ±yor
- **Training**: `_dirhit(y_true, y_pred, thr=0.005)` - aynÄ± mantÄ±k âœ…

## ğŸ¯ KÃ¶k Neden Analizi

### Senaryo 1: Low Support Gating FarkÄ± (EN MUHTEMEL)
**Problem**: Training'de low support kontrolÃ¼ daha sÄ±kÄ± (min_mask_count=10, min_mask_pct=5.0), HPO'da ise default (0, 0).

**Etki**: 
- HPO'da tÃ¼m split'ler deÄŸerlendiriliyor
- Training'de bazÄ± split'ler exclude ediliyor (mask_count < 10 veya mask_pct < 5.0)
- Bu, training DirHit'inin daha dÃ¼ÅŸÃ¼k Ã§Ä±kmasÄ±na neden olabilir

**Ã–rnek**: 
- ADEL_1d: HPO DirHit=85.42% Training DirHit=42.21%
- EÄŸer HPO'da 4 split varsa ve training'de 2 split exclude edilirse, fark bÃ¼yÃ¼k olabilir

### Senaryo 2: Evaluation Spec EksikliÄŸi
**Problem**: Training'de `evaluation_spec` yoksa veya farklÄ±ysa, split'ler farklÄ± olabilir.

**Etki**: 
- HPO'da kullanÄ±lan split'ler ile training'de kullanÄ±lan split'ler farklÄ± olabilir
- Bu, farklÄ± test setleri Ã¼zerinde deÄŸerlendirme yapÄ±lmasÄ±na neden olur

### Senaryo 3: Model State Contamination
**Problem**: Training'de Ã¶nce tÃ¼m df ile model eÄŸitiliyor, sonra evaluation iÃ§in train_df ile yeniden eÄŸitiliyor.

**Etki**: 
- Ä°lk eÄŸitim model state'ini etkileyebilir (singleton cache, global state, vb.)
- Yeni instance oluÅŸturuluyor ama bazÄ± global state'ler temizlenmemiÅŸ olabilir

### Senaryo 4: Seed Bagging FarkÄ±
**Problem**: HPO'da seed bagging aÃ§Ä±k/kapalÄ± olabilir, training'de farklÄ± olabilir.

**Etki**: 
- Seed bagging aÃ§Ä±k/kapalÄ± durumu farklÄ±ysa, model eÄŸitimi farklÄ± olabilir

## ğŸ”§ Ã–nerilen DÃ¼zeltmeler

### 1. Low Support Gating TutarlÄ±lÄ±ÄŸÄ± âœ… DÃœZELTÄ°LDÄ°
```python
# continuous_hpo_training_pipeline.py satÄ±r 2438-2444
# DÃœZELTME YAPILDI:
_min_mc = int(os.getenv('HPO_MIN_MASK_COUNT', '0'))  # Default: 0 (HPO ile aynÄ±)
_min_mp = float(os.getenv('HPO_MIN_MASK_PCT', '0.0'))  # Default: 0.0 (HPO ile aynÄ±)
```

### 2. Evaluation Spec KontrolÃ¼
- HPO JSON'da `evaluation_spec` olup olmadÄ±ÄŸÄ±nÄ± kontrol et
- EÄŸer yoksa, HPO'daki split'leri training'e aktar
- Split'lerin aynÄ± olduÄŸundan emin ol

### 3. Model State TemizliÄŸi
- Evaluation Ã¶ncesi tÃ¼m global state'leri temizle
- ConfigManager cache'i temizle
- Singleton instance'larÄ± sÄ±fÄ±rla

### 4. Seed Bagging KontrolÃ¼
- HPO best trial'da seed bagging aÃ§Ä±k/kapalÄ± durumunu kontrol et
- Training evaluation'da aynÄ± durumu kullan

## ğŸ“ˆ Ã–rnek Vakalar

### Vaka 1: ADEL_1d (HPO: 85.42% â†’ Training: 42.21%)
- **Fark**: -43.21 puan
- **OlasÄ± Neden**: Low support gating veya split farkÄ±

### Vaka 2: BRSAN_1d (HPO: 100.00% â†’ Training: 64.41%)
- **Fark**: -35.59 puan
- **OlasÄ± Neden**: Low support gating (HPO'da tÃ¼m split'ler dahil, training'de bazÄ±larÄ± exclude)

### Vaka 3: EKGYO_1d (HPO: 100.00% â†’ Training: 58.18%)
- **Fark**: -41.82 puan
- **OlasÄ± Neden**: Benzer - low support veya split farkÄ±

## âœ… DoÄŸrulama AdÄ±mlarÄ±

1. **Low Support KontrolÃ¼**: HPO ve training'de aynÄ± threshold'larÄ± kullan
2. **Split KontrolÃ¼**: HPO JSON'dan split'leri al ve training'de kullan
3. **Seed KontrolÃ¼**: Best trial'Ä±n seed'ini doÄŸru kullan
4. **Model State KontrolÃ¼**: Evaluation Ã¶ncesi tÃ¼m state'leri temizle
5. **Logging**: Her adÄ±mda detaylÄ± log tut

## ğŸ¯ SonuÃ§

En muhtemel kÃ¶k neden: **Low Support Gating farkÄ±**. Training'de daha sÄ±kÄ± kontrol (min_mask_count=10, min_mask_pct=5.0) var, HPO'da ise default (0, 0). Bu, training'de bazÄ± split'lerin exclude edilmesine ve DirHit'in dÃ¼ÅŸmesine neden olabilir.

**Ã–neri**: Low support gating'i HPO ile aynÄ± yap (default: 0, 0) ve sonuÃ§larÄ± gÃ¶zlemle.

